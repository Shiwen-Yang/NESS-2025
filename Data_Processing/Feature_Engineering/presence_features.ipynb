{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9bcafa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sys\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pylab as plt\n",
    "import warnings\n",
    "import itertools\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add the grandparent directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../../')))\n",
    "from Utils import FE_helper as FE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54387326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the data\n",
    "train_df_original = pd.read_csv('../../Original_Data/train_2025.csv') \n",
    "train_df = pd.read_csv('../../Original_Data/train_2025.csv') \n",
    "test_df = pd.read_csv('../../Original_Data/test_2025.csv')\n",
    "\n",
    "train_df = FE.add_features(train_df)\n",
    "test_df = FE.add_features(test_df)\n",
    "\n",
    "test_id = test_df['claim_number']\n",
    "train_id = train_df['claim_number']\n",
    "target = train_df['fraud']\n",
    "\n",
    "ignore_var = ['claim_date.is_weekend', 'claim_date.near_holiday', 'fraud']\n",
    "train_df = FE.drop_ignored_columns(train_df, ignore_var)\n",
    "test_df = FE.drop_ignored_columns(test_df, ignore_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01c41da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_low_cardinality_columns(df, threshold=20, dropna=False):\n",
    "    \"\"\"\n",
    "    Filters columns with unique value counts ≤ threshold.\n",
    "    \"\"\"\n",
    "    low_card_cols = [col for col in df.columns if df[col].nunique(dropna=dropna) <= threshold]\n",
    "    # print(f\"Low-cardinality columns (≤ {threshold} unique values): {low_card_cols}\")\n",
    "    return low_card_cols\n",
    "\n",
    "def generate_column_combinations(columns, sizes=[2, 3]):\n",
    "    \"\"\"\n",
    "    Generates all combinations of the given columns at specified sizes.\n",
    "    \"\"\"\n",
    "    combos = []\n",
    "    for k in sizes:\n",
    "        combos.extend(itertools.combinations(columns, k))\n",
    "    print(f\"Generated {len(combos)} combinations (sizes {sizes}).\")\n",
    "    return combos\n",
    "\n",
    "\n",
    "def add_presence_features(df, combos):\n",
    "    \"\"\"\n",
    "    For each column combination, adds a binary feature:\n",
    "    1 if the row's combination appears elsewhere in the dataset, 0 otherwise.\n",
    "    Shows a progress indicator.\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "    total = len(combos)\n",
    "\n",
    "    for i, combo in enumerate(combos, 1):\n",
    "        combo_name = \"__\".join(combo) + \"_present\"\n",
    "        \n",
    "        # Progress indicator\n",
    "        progress_msg = f\"\\rProcessing {i} / {total} combos ({100 * i / total:.2f}%)\"\n",
    "        sys.stdout.write(progress_msg)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Create a tuple column for matching\n",
    "        combo_tuples = df[list(combo)].apply(tuple, axis=1)\n",
    "        # Count how many times each tuple appears\n",
    "        counts = combo_tuples.map(combo_tuples.value_counts())\n",
    "        # Presence = appears more than once\n",
    "        df_out[combo_name] = (counts > 1).astype(int)\n",
    "    \n",
    "    # Final newline to clean up progress line\n",
    "    sys.stdout.write(\"\\nDone!\\n\")\n",
    "\n",
    "    return df_out\n",
    "\n",
    "def generate_all_nonempty_subsets(features):\n",
    "    \"\"\"\n",
    "    Generates all nonempty subsets (combinations) of the given feature list.\n",
    "    \n",
    "    Parameters:\n",
    "    - features (list): List of feature names.\n",
    "    \n",
    "    Returns:\n",
    "    - list of tuples: All nonempty subsets.\n",
    "    \"\"\"\n",
    "    all_subsets = []\n",
    "    for k in range(1, len(features) + 1):\n",
    "        combos = list(itertools.combinations(features, k))\n",
    "        all_subsets.extend(combos)\n",
    "    print(f\"Generated {len(all_subsets)} total nonempty subsets.\")\n",
    "    return all_subsets\n",
    "\n",
    "def compute_fraud_rate_differences(df, target_col='fraud', suffix='_present', variance_threshold=None):\n",
    "    \"\"\"\n",
    "    Computes the difference in fraud rates between rows with feature == 1 and feature == 0\n",
    "    for all columns ending with the given suffix.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame\n",
    "    - target_col (str): Name of the target binary column (e.g., 'fraud')\n",
    "    - suffix (str): Suffix to identify newly added features\n",
    "    - filter_zero_variance (bool): Whether to exclude columns with zero variance\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Feature name, fraud rate at 0, fraud rate at 1, and the difference\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Select columns with the specified suffix\n",
    "    feature_cols = [col for col in df.columns if col.endswith(suffix)]\n",
    "\n",
    "    if variance_threshold is not None:\n",
    "        variances = df[feature_cols].var()\n",
    "        feature_cols = [col for col in feature_cols if variances[col] > variance_threshold]\n",
    "        print(f\"Kept {len(feature_cols)} higher-variance features.\")\n",
    "\n",
    "    for col in feature_cols:\n",
    "        grouped = df.groupby(col)[target_col].mean()\n",
    "        rate_0 = grouped.get(0, None)\n",
    "        rate_1 = grouped.get(1, None)\n",
    "        if rate_0 is not None and rate_1 is not None:\n",
    "            diff = rate_1 - rate_0\n",
    "            var = df[col].var()\n",
    "            results.append({\n",
    "                'feature': col,\n",
    "                'fraud_rate_at_0': rate_0,\n",
    "                'fraud_rate_at_1': rate_1,\n",
    "                'difference': diff,\n",
    "                'variance': var\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results).sort_values(by='difference', key=abs, ascending=False)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a243be03",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_drops = ['vehicle_price_categories', 'zero_payout', 'log_pop_bin', 'age_group', 'past_num_of_claims', 'age_of_vehicle', 'claim_date.weekofyear', 'claim_date.day', 'claim_date.quarter', 'zipcode_type']\n",
    "\n",
    "low_performing_col = train_df[filter_low_cardinality_columns(train_df, threshold=60)].drop(columns = additional_drops).columns.tolist()\n",
    "\n",
    "# Step 2: Generate all 2- and 3-column combinations\n",
    "combos = generate_all_nonempty_subsets(low_performing_col)\n",
    "\n",
    "# Step 3: Add binary presence features\n",
    "df_with_features = add_presence_features(train_df, combos)\n",
    "\n",
    "print(\"New dataset shape:\", df_with_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a65691",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_features['fraud'] = target\n",
    "fraud_diffs = compute_fraud_rate_differences(df_with_features, target_col='fraud', variance_threshold=0.1)\n",
    "temp_1 = fraud_diffs[np.abs(fraud_diffs['difference']) > 0.03]\n",
    "\n",
    "temp_1['info'] = (temp_1['variance']/0.25)*np.abs(temp_1['difference'])\n",
    "temp_1 = temp_1.sort_values(by='info', ascending=False)\n",
    "temp_1.to_csv('../../Create_Tune_Models/logs/subset_info_3.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f2539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_card_col = filter_low_cardinality_columns(train_df, threshold=120)\n",
    "\n",
    "custom_cols = ['claim_date.weekofyear', 'claim_date.quarter', 'log_pop_bin', 'vehicle_price_categories', 'zero_payout']\n",
    "low_card_col = [col for col in low_card_col if col not in custom_cols]\n",
    "\n",
    "\n",
    "# Step 2: Generate all 2- and 3-column combinations\n",
    "combos_2345 = generate_column_combinations(low_card_col, [2, 3, 4, 5])\n",
    "\n",
    "# Step 3: Add binary presence features\n",
    "df_with_features_2345 = add_presence_features(train_df, combos_2345)\n",
    "\n",
    "print(\"New dataset shape:\", df_with_features_2345.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e40676",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_features_2345['fraud'] = target\n",
    "fraud_diffs_2345 = compute_fraud_rate_differences(df_with_features_2345, target_col='fraud', variance_threshold=0.1)\n",
    "temp_2 = fraud_diffs_2345[np.abs(fraud_diffs_2345['difference']) > 0.03]\n",
    "\n",
    "temp_2['info'] = (temp_2['variance']/0.25)*np.abs(temp_2['difference'])\n",
    "temp_2 = temp_2.sort_values(by='info', ascending=False)\n",
    "temp_2.to_csv('../../Create_Tune_Models/logs/subset_info_2.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
