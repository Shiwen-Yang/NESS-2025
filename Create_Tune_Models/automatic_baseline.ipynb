{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "499c1ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import library\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2784c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", message=\".*load_learner.*insecure pickle.*\")\n",
    "\n",
    "# 2. Load the data\n",
    "train_df = pd.read_csv('Data/original/train_2025.csv')    # Adjust path if needed\n",
    "test_df = pd.read_csv('Data/original/test_2025.csv')      # Adjust path if needed\n",
    "\n",
    "train_df.drop(columns=\"claim_number\", inplace=True)\n",
    "test_id = test_df['claim_number']\n",
    "test_df.drop(columns=[\"claim_number\", \"fraud\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "401332eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250426_135506\"\n",
      "Preset alias specified: 'best' maps to 'best_quality'.\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.11.12\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          16\n",
      "Memory Avail:       16.59 GB / 31.93 GB (52.0%)\n",
      "Disk Space Avail:   267.97 GB / 935.97 GB (28.6%)\n",
      "===================================================\n",
      "Presets specified: ['best']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n",
      "2025-04-26 09:55:07,656\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-04-26 09:55:10,940\tINFO worker.py:1810 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"c:\\Users\\yangs\\Documents\\Python Projects\\NESS-2025\\AutogluonModels\\ag-20250426_135506\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Beginning AutoGluon training ... Time limit = 894s\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m AutoGluon will save models to \"c:\\Users\\yangs\\Documents\\Python Projects\\NESS-2025\\AutogluonModels\\ag-20250426_135506\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Train Data Rows:    16000\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Train Data Columns: 23\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Label Column:       fraud\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Problem Type:       binary\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tAvailable Memory:                    16347.72 MB\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tTrain Data (Original)  Memory Usage: 9.46 MB (0.1% of available memory)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t\tFitting DatetimeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t\t('float', [])                      :  5 | ['marital_status', 'witness_present_ind', 'claim_est_payout', 'vehicle_price', 'vehicle_weight']\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t\t('int', [])                        : 10 | ['age_of_driver', 'safty_rating', 'annual_income', 'high_education_ind', 'address_change_ind', ...]\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t\t('object', [])                     :  7 | ['gender', 'living_status', 'claim_day_of_week', 'accident_site', 'channel', ...]\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t\t('object', ['datetime_as_object']) :  1 | ['claim_date']\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t\t('category', [])             : 5 | ['claim_day_of_week', 'accident_site', 'channel', 'vehicle_category', 'vehicle_color']\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t\t('float', [])                : 5 | ['marital_status', 'witness_present_ind', 'claim_est_payout', 'vehicle_price', 'vehicle_weight']\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t\t('int', [])                  : 7 | ['age_of_driver', 'safty_rating', 'annual_income', 'zip_code', 'past_num_of_claims', ...]\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t\t('int', ['bool'])            : 5 | ['gender', 'high_education_ind', 'address_change_ind', 'living_status', 'policy_report_filed_ind']\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t\t('int', ['datetime_as_int']) : 5 | ['claim_date', 'claim_date.year', 'claim_date.month', 'claim_date.day', 'claim_date.dayofweek']\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.1s = Fit runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t23 features in original data used to generate 27 features in processed data.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tTrain Data (Processed) Memory Usage: 2.23 MB (0.0% of available memory)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Data preprocessing and feature engineering runtime = 0.14s ...\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'f1'\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 595.90s of the 894.07s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0656\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.01s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.14s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 593.40s of the 891.57s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0656\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.02s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.18s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 593.16s of the 891.32s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.13%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=25660)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.420321\tvalid_set's f1: 0.0793201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.077\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t7.0s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.33s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 582.62s of the 880.79s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.13%)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0889\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t4.75s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.22s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 574.77s of the 872.94s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0032\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.93s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.45s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 573.20s of the 871.37s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0024\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t1.04s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.38s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 571.61s of the 869.78s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.16%)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0163\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t41.4s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 527.22s of the 825.39s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0055\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.69s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.49s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 525.80s of the 823.97s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0039\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.69s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.41s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 524.51s of the 822.68s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.17%)\n",
      "\u001b[36m(_ray_fit pid=23940)\u001b[0m No improvement since epoch 0: early stopping\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.2867\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t16.27s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.3s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 505.21s of the 803.38s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.24%)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.1138\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t10.36s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.43s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 491.09s of the 789.26s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.09%)\n",
      "\u001b[36m(_ray_fit pid=26808)\u001b[0m No improvement since epoch 0: early stopping\u001b[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.2674\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t195.88s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.19s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 292.22s of the 590.39s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.23%)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0474\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t8.12s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.24s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 280.60s of the 578.77s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.17%)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0209\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t4.65s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 272.74s of the 570.91s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.10%)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.2504\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t108.12s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.2s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 161.06s of the 459.23s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.16%)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0016\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t3.63s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 154.05s of the 452.21s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.17%)\n",
      "\u001b[36m(_ray_fit pid=32140)\u001b[0m No improvement since epoch 0: early stopping\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.2842\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t37.21s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.48s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 113.21s of the 411.38s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.44%)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0224\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t39.19s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.06s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 70.26s of the 368.43s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.11%)\n",
      "\u001b[36m(_ray_fit pid=32532)\u001b[0m No improvement since epoch 0: early stopping\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t1.48s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.02s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 65.84s of the 364.01s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.09%)\n",
      "\u001b[36m(_ray_fit pid=8240)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 78)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.2149\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t55.54s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.28s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 6.98s of the 305.15s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.75%)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0171\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t5.76s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.35s\t = Validation runtime\n",
      "\u001b[36m(_ray_fit pid=32824)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 78)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 295.46s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tEnsemble Weights: {'NeuralNetFastAI_BAG_L1': 0.615, 'NeuralNetFastAI_r191_BAG_L1': 0.385}\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.2977\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t2.24s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.01s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting 108 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 293.19s of the 293.12s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.20%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=20040)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.43524\tvalid_set's f1: 0.0788732\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0683\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t9.62s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.32s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 280.42s of the 280.35s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.20%)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0904\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t6.09s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.22s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 271.04s of the 270.97s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0343\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t1.62s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.51s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 268.74s of the 268.66s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0283\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t1.95s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.47s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 266.16s of the 266.09s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.26%)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0224\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t33.58s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.05s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 229.71s of the 229.63s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0208\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.7s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.5s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 228.30s of the 228.22s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0208\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.72s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.51s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 226.86s of the 226.79s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.25%)\n",
      "\u001b[36m(_ray_fit pid=25696)\u001b[0m No improvement since epoch 0: early stopping\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.3503\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t18.5s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.33s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 205.43s of the 205.35s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.38%)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.1259\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t14.22s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.35s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 187.19s of the 187.11s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.14%)\n",
      "\u001b[36m(_ray_fit pid=20516)\u001b[0m No improvement since epoch 0: early stopping\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.2915\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t81.95s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.26s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 101.87s of the 101.79s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.39%)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0692\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t13.21s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.3s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 85.18s of the 85.11s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.27%)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0231\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t6.62s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.06s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 75.40s of the 75.33s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.15%)\n",
      "\u001b[36m(_ray_fit pid=26504)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 104)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.2823\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t63.0s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.29s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 9.23s of the 9.16s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.27%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=28792)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.413901\tvalid_set's f1: 0.0647059\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=14340)\u001b[0m \tRan out of time, early stopping on iteration 1096. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=14340)\u001b[0m \t[1025]\tvalid_set's binary_logloss: 0.404287\tvalid_set's f1: 0.0762463\n",
      "\u001b[36m(_ray_fit pid=21172)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 103)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.0474\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t7.77s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.36s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -2.42s of remaining time.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tEnsemble Weights: {'NeuralNetFastAI_BAG_L2': 0.474, 'NeuralNetFastAI_BAG_L1': 0.263, 'NeuralNetFastAI_r191_BAG_L1': 0.158, 'NeuralNetTorch_BAG_L2': 0.105}\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.3604\t = Validation score   (f1)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t3.19s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \t0.01s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m AutoGluon training complete, total runtime = 899.86s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 488.3 rows/s (2000 batch size)\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Calibrating decision threshold to optimize metric f1 | Checking 51 thresholds...\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tBase Threshold: 0.500\t| val: 0.3604\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tBest Threshold: 0.496\t| val: 0.3615\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Updating predictor.decision_threshold from 0.5 -> 0.496\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tThis will impact how prediction probabilities are converted to predictions in binary classification.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tPrediction probabilities of the positive class >0.496 will be predicted as the positive class (1). This can significantly impact metric scores.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tYou can update this value via `predictor.set_decision_threshold`.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m \tYou can calculate an optimal decision threshold on the validation data via `predictor.calibrate_decision_threshold()`.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\yangs\\Documents\\Python Projects\\NESS-2025\\AutogluonModels\\ag-20250426_135506\\ds_sub_fit\\sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_ray_fit pid=684)\u001b[0m \tRan out of time, early stopping on iteration 1103. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=684)\u001b[0m \t[1071]\tvalid_set's binary_logloss: 0.409661\tvalid_set's f1: 0.0823529\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001b[36m(_dystack pid=30184)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                          model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0        NeuralNetFastAI_BAG_L2       0.372302   0.350280          f1        6.785181       5.625957  561.246446                 0.464500                0.328880          18.504014            2       True         30\n",
      "1           WeightedEnsemble_L3       0.360092   0.360447          f1        7.269191       5.891225  646.394574                 0.013124                0.006124           3.193642            3       True         37\n",
      "2           WeightedEnsemble_L2       0.314607   0.297733          f1        1.443632       0.789022   55.715496                 0.014163                0.006001           2.239101            2       True         22\n",
      "3        NeuralNetFastAI_BAG_L1       0.307544   0.286677          f1        0.898495       0.302501   16.269422                 0.898495                0.302501          16.269422            1       True         10\n",
      "4   NeuralNetFastAI_r191_BAG_L1       0.306283   0.284245          f1        0.530974       0.480520   37.206973                 0.530974                0.480520          37.206973            1       True         17\n",
      "5         NeuralNetTorch_BAG_L2       0.266160   0.291521          f1        6.791568       5.556221  624.696918                 0.470887                0.259144          81.954485            2       True         32\n",
      "6         NeuralNetTorch_BAG_L1       0.223801   0.267402          f1        0.284747       0.186351  195.882962                 0.284747                0.186351         195.882962            1       True         12\n",
      "7     NeuralNetTorch_r79_BAG_L2       0.195556   0.282301          f1        6.680470       5.587423  605.738577                 0.359790                0.290345          62.996145            2       True         35\n",
      "8     NeuralNetTorch_r22_BAG_L1       0.175676   0.214896          f1        0.315623       0.282828   55.541575                 0.315623                0.282828          55.541575            1       True         20\n",
      "9     NeuralNetTorch_r79_BAG_L1       0.165899   0.250445          f1        0.296010       0.204391  108.116806                 0.296010                0.204391         108.116806            1       True         15\n",
      "10               XGBoost_BAG_L1       0.105556   0.113810          f1        0.575510       0.426784   10.358795                 0.575510                0.426784          10.358795            1       True         11\n",
      "11        KNeighborsUnif_BAG_L1       0.062663   0.065595          f1        0.052898       0.139762    0.012527                 0.052898                0.139762           0.012527            1       True          1\n",
      "12        KNeighborsDist_BAG_L1       0.062663   0.065595          f1        0.053245       0.183271    0.017232                 0.053245                0.183271           0.017232            1       True          2\n",
      "13            LightGBMXT_BAG_L1       0.058140   0.077034          f1        0.469567       0.329639    7.004568                 0.469567                0.329639           7.004568            1       True          3\n",
      "14              LightGBM_BAG_L1       0.047478   0.088936          f1        0.234375       0.222942    4.748924                 0.234375                0.222942           4.748924            1       True          4\n",
      "15               XGBoost_BAG_L2       0.046647   0.125850          f1        6.807651       5.649819  556.958178                 0.486970                0.352741          14.215745            2       True         31\n",
      "16            LightGBMXT_BAG_L2       0.036036   0.068307          f1        6.593127       5.616960  552.360419                 0.272446                0.319883           9.617986            2       True         23\n",
      "17              LightGBM_BAG_L2       0.024390   0.090359          f1        6.539493       5.520708  548.829113                 0.218812                0.223630           6.086680            2       True         24\n",
      "18      RandomForestEntr_BAG_L2       0.018576   0.028287          f1        6.485983       5.764599  544.688263                 0.165302                0.467522           1.945830            2       True         26\n",
      "19         LightGBMLarge_BAG_L1       0.018349   0.047368          f1        0.269891       0.244032    8.115189                 0.269891                0.244032           8.115189            1       True         13\n",
      "20         CatBoost_r177_BAG_L1       0.012461   0.020898          f1        0.110358       0.044673    4.651921                 0.110358                0.044673           4.651921            1       True         14\n",
      "21           CatBoost_r9_BAG_L1       0.012461   0.022359          f1        0.124910       0.064607   39.190709                 0.124910                0.064607          39.190709            1       True         18\n",
      "22              CatBoost_BAG_L1       0.012461   0.016298          f1        0.560559       0.041342   41.400303                 0.560559                0.041342          41.400303            1       True          7\n",
      "23        ExtraTreesGini_BAG_L2       0.012422   0.020849          f1        6.521108       5.796872  543.444911                 0.200427                0.499795           0.702479            2       True         28\n",
      "24         LightGBMLarge_BAG_L2       0.012346   0.069245          f1        6.648154       5.598184  555.953203                 0.327474                0.301107          13.210771            2       True         33\n",
      "25      RandomForestGini_BAG_L1       0.006309   0.003151          f1        0.207006       0.449740    0.934528                 0.207006                0.449740           0.934528            1       True          5\n",
      "26         CatBoost_r177_BAG_L2       0.006309   0.023095          f1        6.443850       5.357195  549.361956                 0.123169                0.060117           6.619524            2       True         34\n",
      "27        ExtraTreesEntr_BAG_L2       0.006289   0.020825          f1        6.526922       5.803702  543.465762                 0.206242                0.506625           0.723330            2       True         29\n",
      "28      RandomForestGini_BAG_L2       0.006192   0.034325          f1        6.510528       5.802408  544.362564                 0.189847                0.505331           1.620131            2       True         25\n",
      "29          LightGBM_r96_BAG_L1       0.000000   0.000000          f1        0.104022       0.024539    1.478166                 0.104022                0.024539           1.478166            1       True         19\n",
      "30         LightGBM_r131_BAG_L1       0.000000   0.001577          f1        0.131709       0.044748    3.626958                 0.131709                0.044748           3.626958            1       True         16\n",
      "31      RandomForestEntr_BAG_L1       0.000000   0.002363          f1        0.185326       0.379617    1.043947                 0.185326                0.379617           1.043947            1       True          6\n",
      "32        ExtraTreesEntr_BAG_L1       0.000000   0.003929          f1        0.213896       0.405486    0.689840                 0.213896                0.405486           0.689840            1       True          9\n",
      "33        ExtraTreesGini_BAG_L1       0.000000   0.005503          f1        0.221650       0.488092    0.686418                 0.221650                0.488092           0.686418            1       True          8\n",
      "34           XGBoost_r33_BAG_L1       0.000000   0.017061          f1        0.479911       0.351209    5.764672                 0.479911                0.351209           5.764672            1       True         21\n",
      "35              CatBoost_BAG_L2       0.000000   0.022351          f1        6.422824       5.346922  576.324297                 0.102144                0.049845          33.581864            2       True         27\n",
      "36         LightGBM_r131_BAG_L2       0.000000   0.047440          f1        6.614621       5.658247  550.515070                 0.293940                0.361170           7.772637            2       True         36\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t919s\t = DyStack   runtime |\t2681s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 2681s\n",
      "AutoGluon will save models to \"c:\\Users\\yangs\\Documents\\Python Projects\\NESS-2025\\AutogluonModels\\ag-20250426_135506\"\n",
      "Train Data Rows:    18000\n",
      "Train Data Columns: 23\n",
      "Label Column:       fraud\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    15796.42 MB\n",
      "\tTrain Data (Original)  Memory Usage: 10.64 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      :  5 | ['marital_status', 'witness_present_ind', 'claim_est_payout', 'vehicle_price', 'vehicle_weight']\n",
      "\t\t('int', [])                        : 10 | ['age_of_driver', 'safty_rating', 'annual_income', 'high_education_ind', 'address_change_ind', ...]\n",
      "\t\t('object', [])                     :  7 | ['gender', 'living_status', 'claim_day_of_week', 'accident_site', 'channel', ...]\n",
      "\t\t('object', ['datetime_as_object']) :  1 | ['claim_date']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])             : 5 | ['claim_day_of_week', 'accident_site', 'channel', 'vehicle_category', 'vehicle_color']\n",
      "\t\t('float', [])                : 5 | ['marital_status', 'witness_present_ind', 'claim_est_payout', 'vehicle_price', 'vehicle_weight']\n",
      "\t\t('int', [])                  : 7 | ['age_of_driver', 'safty_rating', 'annual_income', 'zip_code', 'past_num_of_claims', ...]\n",
      "\t\t('int', ['bool'])            : 5 | ['gender', 'high_education_ind', 'address_change_ind', 'living_status', 'policy_report_filed_ind']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['claim_date', 'claim_date.year', 'claim_date.month', 'claim_date.day', 'claim_date.dayofweek']\n",
      "\t0.2s = Fit runtime\n",
      "\t23 features in original data used to generate 27 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 2.51 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.2s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 1786.84s of the 2680.92s of remaining time.\n",
      "\t0.0427\t = Validation score   (f1)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 1783.76s of the 2677.84s of remaining time.\n",
      "\t0.0427\t = Validation score   (f1)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1783.48s of the 2677.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.13%)\n",
      "\t0.0848\t = Validation score   (f1)\n",
      "\t11.22s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1768.87s of the 2662.95s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.13%)\n",
      "\t0.0836\t = Validation score   (f1)\n",
      "\t4.26s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 1761.43s of the 2655.51s of remaining time.\n",
      "\t0.0028\t = Validation score   (f1)\n",
      "\t1.11s\t = Training   runtime\n",
      "\t0.5s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 1759.61s of the 2653.69s of remaining time.\n",
      "\t0.0035\t = Validation score   (f1)\n",
      "\t1.12s\t = Training   runtime\n",
      "\t0.5s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 1757.78s of the 2651.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.17%)\n",
      "\t0.0097\t = Validation score   (f1)\n",
      "\t35.96s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 1718.68s of the 2612.76s of remaining time.\n",
      "\t0.0042\t = Validation score   (f1)\n",
      "\t0.76s\t = Training   runtime\n",
      "\t0.55s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 1717.14s of the 2611.22s of remaining time.\n",
      "\t0.0028\t = Validation score   (f1)\n",
      "\t0.78s\t = Training   runtime\n",
      "\t0.54s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1715.56s of the 2609.64s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.17%)\n",
      "\t0.2853\t = Validation score   (f1)\n",
      "\t19.71s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 1692.67s of the 2586.75s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.24%)\n",
      "\t0.1054\t = Validation score   (f1)\n",
      "\t10.04s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 1678.62s of the 2572.70s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.10%)\n",
      "\t0.2587\t = Validation score   (f1)\n",
      "\t186.03s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1489.50s of the 2383.58s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.23%)\n",
      "\t0.0564\t = Validation score   (f1)\n",
      "\t7.52s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 1478.33s of the 2372.41s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.16%)\n",
      "\t0.0165\t = Validation score   (f1)\n",
      "\t4.42s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 1470.72s of the 2364.80s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.10%)\n",
      "\t0.2507\t = Validation score   (f1)\n",
      "\t178.62s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 1288.58s of the 2182.66s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.16%)\n",
      "\t0.0063\t = Validation score   (f1)\n",
      "\t5.09s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 1280.05s of the 2174.13s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.17%)\n",
      "\t0.2828\t = Validation score   (f1)\n",
      "\t42.16s\t = Training   runtime\n",
      "\t0.51s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 1234.56s of the 2128.64s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.45%)\n",
      "\t0.0138\t = Validation score   (f1)\n",
      "\t44.47s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 1185.97s of the 2080.05s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.12%)\n",
      "\t0.0\t = Validation score   (f1)\n",
      "\t1.62s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 1181.02s of the 2075.10s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.10%)\n",
      "\t0.2394\t = Validation score   (f1)\n",
      "\t205.4s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 971.77s of the 1865.85s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.66%)\n",
      "\t0.0363\t = Validation score   (f1)\n",
      "\t15.37s\t = Training   runtime\n",
      "\t1.01s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 952.97s of the 1847.05s of remaining time.\n",
      "\t0.0244\t = Validation score   (f1)\n",
      "\t1.08s\t = Training   runtime\n",
      "\t0.5s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 951.18s of the 1845.26s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.12%)\n",
      "\t0.0152\t = Validation score   (f1)\n",
      "\t25.46s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 922.45s of the 1816.53s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.18%)\n",
      "\t0.2268\t = Validation score   (f1)\n",
      "\t10.72s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 908.43s of the 1802.50s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.45%)\n",
      "\t0.0083\t = Validation score   (f1)\n",
      "\t16.28s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 888.10s of the 1782.18s of remaining time.\n",
      "\t0.0319\t = Validation score   (f1)\n",
      "\t2.97s\t = Training   runtime\n",
      "\t0.53s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 884.40s of the 1778.48s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.23%)\n",
      "\t0.0614\t = Validation score   (f1)\n",
      "\t9.66s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 871.04s of the 1765.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.18%)\n",
      "\t0.2543\t = Validation score   (f1)\n",
      "\t41.03s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 826.51s of the 1720.59s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.25%)\n",
      "\t0.1144\t = Validation score   (f1)\n",
      "\t14.25s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 808.23s of the 1702.31s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.10%)\n",
      "\t0.2477\t = Validation score   (f1)\n",
      "\t135.47s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 669.59s of the 1563.67s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.19%)\n",
      "\t0.0872\t = Validation score   (f1)\n",
      "\t7.5s\t = Training   runtime\n",
      "\t0.43s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r86_BAG_L1 ... Training model for up to 658.43s of the 1552.51s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.09%)\n",
      "\t0.2595\t = Validation score   (f1)\n",
      "\t117.65s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: CatBoost_r50_BAG_L1 ... Training model for up to 537.46s of the 1431.54s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.12%)\n",
      "\t0.0293\t = Validation score   (f1)\n",
      "\t18.44s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r11_BAG_L1 ... Training model for up to 515.63s of the 1409.71s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.17%)\n",
      "\t0.3151\t = Validation score   (f1)\n",
      "\t86.74s\t = Training   runtime\n",
      "\t0.81s\t = Validation runtime\n",
      "Fitting model: XGBoost_r194_BAG_L1 ... Training model for up to 425.50s of the 1319.58s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.28%)\n",
      "\t0.1084\t = Validation score   (f1)\n",
      "\t14.5s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r172_BAG_L1 ... Training model for up to 407.32s of the 1301.40s of remaining time.\n",
      "\t0.0212\t = Validation score   (f1)\n",
      "\t1.14s\t = Training   runtime\n",
      "\t0.56s\t = Validation runtime\n",
      "Fitting model: CatBoost_r69_BAG_L1 ... Training model for up to 405.45s of the 1299.53s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.13%)\n",
      "\t0.0166\t = Validation score   (f1)\n",
      "\t35.09s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r103_BAG_L1 ... Training model for up to 367.28s of the 1261.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.17%)\n",
      "\t0.2216\t = Validation score   (f1)\n",
      "\t36.45s\t = Training   runtime\n",
      "\t0.4s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r14_BAG_L1 ... Training model for up to 327.25s of the 1221.33s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.10%)\n",
      "\t0.2529\t = Validation score   (f1)\n",
      "\t70.22s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: LightGBM_r161_BAG_L1 ... Training model for up to 253.28s of the 1147.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.35%)\n",
      "\t0.0035\t = Validation score   (f1)\n",
      "\t7.25s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r143_BAG_L1 ... Training model for up to 242.19s of the 1136.27s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.17%)\n",
      "\t0.0591\t = Validation score   (f1)\n",
      "\t14.76s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: CatBoost_r70_BAG_L1 ... Training model for up to 224.16s of the 1118.24s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.16%)\n",
      "\t0.0325\t = Validation score   (f1)\n",
      "\t40.58s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r156_BAG_L1 ... Training model for up to 180.23s of the 1074.31s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.18%)\n",
      "\t0.3094\t = Validation score   (f1)\n",
      "\t12.24s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: LightGBM_r196_BAG_L1 ... Training model for up to 164.71s of the 1058.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.29%)\n",
      "\t0.0\t = Validation score   (f1)\n",
      "\t2.67s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForest_r39_BAG_L1 ... Training model for up to 158.06s of the 1052.14s of remaining time.\n",
      "\t0.027\t = Validation score   (f1)\n",
      "\t3.23s\t = Training   runtime\n",
      "\t0.53s\t = Validation runtime\n",
      "Fitting model: CatBoost_r167_BAG_L1 ... Training model for up to 154.12s of the 1048.20s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.23%)\n",
      "\t0.0152\t = Validation score   (f1)\n",
      "\t34.57s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r95_BAG_L1 ... Training model for up to 116.29s of the 1010.37s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.17%)\n",
      "\t0.207\t = Validation score   (f1)\n",
      "\t61.77s\t = Training   runtime\n",
      "\t0.56s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r41_BAG_L1 ... Training model for up to 50.89s of the 944.97s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.11%)\n",
      "\t0.1833\t = Validation score   (f1)\n",
      "\t44.02s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: XGBoost_r98_BAG_L1 ... Training model for up to 2.86s of the 896.94s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.45%)\n",
      "\t0.0\t = Validation score   (f1)\n",
      "\t2.4s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 889.65s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L1': 0.381, 'NeuralNetFastAI_r11_BAG_L1': 0.286, 'NeuralNetFastAI_r156_BAG_L1': 0.286, 'NeuralNetFastAI_r191_BAG_L1': 0.048}\n",
      "\t0.3293\t = Validation score   (f1)\n",
      "\t4.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 108 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 885.57s of the 885.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.32%)\n",
      "\t0.0793\t = Validation score   (f1)\n",
      "\t11.8s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 870.22s of the 870.07s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.32%)\n",
      "\t0.0951\t = Validation score   (f1)\n",
      "\t10.69s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 855.22s of the 855.08s of remaining time.\n",
      "\t0.046\t = Validation score   (f1)\n",
      "\t3.09s\t = Training   runtime\n",
      "\t0.76s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 851.16s of the 851.01s of remaining time.\n",
      "\t0.0474\t = Validation score   (f1)\n",
      "\t3.58s\t = Training   runtime\n",
      "\t0.68s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 846.70s of the 846.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.41%)\n",
      "\t0.0097\t = Validation score   (f1)\n",
      "\t40.98s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 802.46s of the 802.32s of remaining time.\n",
      "\t0.028\t = Validation score   (f1)\n",
      "\t0.79s\t = Training   runtime\n",
      "\t0.72s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 800.69s of the 800.55s of remaining time.\n",
      "\t0.0219\t = Validation score   (f1)\n",
      "\t0.77s\t = Training   runtime\n",
      "\t0.74s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 798.94s of the 798.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.41%)\n",
      "\t0.3404\t = Validation score   (f1)\n",
      "\t21.98s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 773.71s of the 773.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.60%)\n",
      "\t0.1057\t = Validation score   (f1)\n",
      "\t13.45s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 756.14s of the 756.00s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.23%)\n",
      "\t0.2806\t = Validation score   (f1)\n",
      "\t56.6s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 695.78s of the 695.64s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.59%)\n",
      "\t0.0784\t = Validation score   (f1)\n",
      "\t18.23s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 673.77s of the 673.63s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.40%)\n",
      "\t0.0206\t = Validation score   (f1)\n",
      "\t9.23s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 661.00s of the 660.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.23%)\n",
      "\t0.2648\t = Validation score   (f1)\n",
      "\t140.01s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 517.14s of the 517.00s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.38%)\n",
      "\t0.0338\t = Validation score   (f1)\n",
      "\t9.98s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L2 ... Training model for up to 503.47s of the 503.33s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.37%)\n",
      "\t0.3274\t = Validation score   (f1)\n",
      "\t48.26s\t = Training   runtime\n",
      "\t0.65s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L2 ... Training model for up to 451.93s of the 451.78s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.93%)\n",
      "\t0.0271\t = Validation score   (f1)\n",
      "\t80.93s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L2 ... Training model for up to 367.67s of the 367.52s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.27%)\n",
      "\t0.0\t = Validation score   (f1)\n",
      "\t1.94s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L2 ... Training model for up to 362.47s of the 362.33s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.22%)\n",
      "\t0.2639\t = Validation score   (f1)\n",
      "\t114.78s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "Fitting model: XGBoost_r33_BAG_L2 ... Training model for up to 243.99s of the 243.85s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.70%)\n",
      "\t0.0703\t = Validation score   (f1)\n",
      "\t43.49s\t = Training   runtime\n",
      "\t1.02s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r42_BAG_L2 ... Training model for up to 196.99s of the 196.85s of remaining time.\n",
      "\t0.0454\t = Validation score   (f1)\n",
      "\t1.94s\t = Training   runtime\n",
      "\t0.62s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L2 ... Training model for up to 194.23s of the 194.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.26%)\n",
      "\t0.0172\t = Validation score   (f1)\n",
      "\t22.48s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L2 ... Training model for up to 168.54s of the 168.40s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.38%)\n",
      "\t0.3308\t = Validation score   (f1)\n",
      "\t9.88s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: CatBoost_r13_BAG_L2 ... Training model for up to 155.48s of the 155.34s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.08%)\n",
      "\t0.0104\t = Validation score   (f1)\n",
      "\t67.29s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForest_r195_BAG_L2 ... Training model for up to 84.57s of the 84.43s of remaining time.\n",
      "\t0.0708\t = Validation score   (f1)\n",
      "\t17.96s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L2 ... Training model for up to 65.88s of the 65.74s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.55%)\n",
      "\t0.0645\t = Validation score   (f1)\n",
      "\t13.1s\t = Training   runtime\n",
      "\t0.52s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L2 ... Training model for up to 49.37s of the 49.23s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.38%)\n",
      "\t0.3116\t = Validation score   (f1)\n",
      "\t43.72s\t = Training   runtime\n",
      "\t0.7s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 1.40s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L2': 0.636, 'NeuralNetFastAI_r11_BAG_L1': 0.136, 'NeuralNetFastAI_r191_BAG_L2': 0.136, 'NeuralNetTorch_BAG_L2': 0.045, 'NeuralNetFastAI_r102_BAG_L2': 0.045}\n",
      "\t0.3543\t = Validation score   (f1)\n",
      "\t3.74s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2683.51s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 232.3 rows/s (2250 batch size)\n",
      "Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
      "Calibrating decision threshold to optimize metric f1 | Checking 51 thresholds...\n",
      "Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
      "\tBase Threshold: 0.500\t| val: 0.3543\n",
      "\tBest Threshold: 0.443\t| val: 0.3610\n",
      "Updating predictor.decision_threshold from 0.5 -> 0.44300000000000006\n",
      "\tThis will impact how prediction probabilities are converted to predictions in binary classification.\n",
      "\tPrediction probabilities of the positive class >0.44300000000000006 will be predicted as the positive class (1). This can significantly impact metric scores.\n",
      "\tYou can update this value via `predictor.set_decision_threshold`.\n",
      "\tYou can calculate an optimal decision threshold on the validation data via `predictor.calibrate_decision_threshold()`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\yangs\\Documents\\Python Projects\\NESS-2025\\AutogluonModels\\ag-20250426_135506\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(\n",
    "    label=\"fraud\",\n",
    "    eval_metric=\"f1\",\n",
    "    problem_type=\"binary\"\n",
    ").fit(\n",
    "    train_data=train_df,\n",
    "    holdout_frac=0.2,\n",
    "    presets=\"best\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0d3effa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WeightedEnsemble_L3</td>\n",
       "      <td>0.354347</td>\n",
       "      <td>f1</td>\n",
       "      <td>13.738319</td>\n",
       "      <td>1263.271420</td>\n",
       "      <td>0.003999</td>\n",
       "      <td>3.735987</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NeuralNetFastAI_BAG_L2</td>\n",
       "      <td>0.340444</td>\n",
       "      <td>f1</td>\n",
       "      <td>12.535254</td>\n",
       "      <td>1144.790274</td>\n",
       "      <td>0.385176</td>\n",
       "      <td>21.977843</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NeuralNetFastAI_r102_BAG_L2</td>\n",
       "      <td>0.330823</td>\n",
       "      <td>f1</td>\n",
       "      <td>12.354052</td>\n",
       "      <td>1132.691576</td>\n",
       "      <td>0.203974</td>\n",
       "      <td>9.879144</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.329344</td>\n",
       "      <td>f1</td>\n",
       "      <td>1.888623</td>\n",
       "      <td>164.877765</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>4.035510</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NeuralNetFastAI_r191_BAG_L2</td>\n",
       "      <td>0.327439</td>\n",
       "      <td>f1</td>\n",
       "      <td>12.802109</td>\n",
       "      <td>1171.076867</td>\n",
       "      <td>0.652030</td>\n",
       "      <td>48.264436</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>RandomForestGini_BAG_L1</td>\n",
       "      <td>0.002795</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.498071</td>\n",
       "      <td>1.113853</td>\n",
       "      <td>0.498071</td>\n",
       "      <td>1.113853</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>LightGBM_r96_BAG_L1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.029065</td>\n",
       "      <td>1.624056</td>\n",
       "      <td>0.029065</td>\n",
       "      <td>1.624056</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>LightGBM_r196_BAG_L1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.029551</td>\n",
       "      <td>2.669791</td>\n",
       "      <td>0.029551</td>\n",
       "      <td>2.669791</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>XGBoost_r98_BAG_L1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.096112</td>\n",
       "      <td>2.404294</td>\n",
       "      <td>0.096112</td>\n",
       "      <td>2.404294</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>LightGBM_r96_BAG_L2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>f1</td>\n",
       "      <td>12.199672</td>\n",
       "      <td>1124.753060</td>\n",
       "      <td>0.049594</td>\n",
       "      <td>1.940629</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model  score_val eval_metric  pred_time_val  \\\n",
       "0           WeightedEnsemble_L3   0.354347          f1      13.738319   \n",
       "1        NeuralNetFastAI_BAG_L2   0.340444          f1      12.535254   \n",
       "2   NeuralNetFastAI_r102_BAG_L2   0.330823          f1      12.354052   \n",
       "3           WeightedEnsemble_L2   0.329344          f1       1.888623   \n",
       "4   NeuralNetFastAI_r191_BAG_L2   0.327439          f1      12.802109   \n",
       "..                          ...        ...         ...            ...   \n",
       "72      RandomForestGini_BAG_L1   0.002795          f1       0.498071   \n",
       "73          LightGBM_r96_BAG_L1   0.000000          f1       0.029065   \n",
       "74         LightGBM_r196_BAG_L1   0.000000          f1       0.029551   \n",
       "75           XGBoost_r98_BAG_L1   0.000000          f1       0.096112   \n",
       "76          LightGBM_r96_BAG_L2   0.000000          f1      12.199672   \n",
       "\n",
       "       fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  \\\n",
       "0   1263.271420                0.003999           3.735987            3   \n",
       "1   1144.790274                0.385176          21.977843            2   \n",
       "2   1132.691576                0.203974           9.879144            2   \n",
       "3    164.877765                0.003509           4.035510            2   \n",
       "4   1171.076867                0.652030          48.264436            2   \n",
       "..          ...                     ...                ...          ...   \n",
       "72     1.113853                0.498071           1.113853            1   \n",
       "73     1.624056                0.029065           1.624056            1   \n",
       "74     2.669791                0.029551           2.669791            1   \n",
       "75     2.404294                0.096112           2.404294            1   \n",
       "76  1124.753060                0.049594           1.940629            2   \n",
       "\n",
       "    can_infer  fit_order  \n",
       "0        True         77  \n",
       "1        True         58  \n",
       "2        True         72  \n",
       "3        True         50  \n",
       "4        True         65  \n",
       "..        ...        ...  \n",
       "72       True          5  \n",
       "73       True         19  \n",
       "74       True         44  \n",
       "75       True         49  \n",
       "76       True         67  \n",
       "\n",
       "[77 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03785190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Predict on the test set\n",
    "predictions = predictor.predict(test_df)\n",
    "\n",
    "test_df = pd.read_csv('Data/original/test_2025.csv')\n",
    "# 5. Save predictions to CSV\n",
    "submission = pd.DataFrame({\n",
    "    \"claim_number\": test_id,  # Important: use the original claim_number\n",
    "    \"fraud\": predictions                      # Your predicted fraud labels (0 or 1)\n",
    "})\n",
    "submission.to_csv(\"submissions/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
