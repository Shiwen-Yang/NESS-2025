{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dfe1b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sys\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pylab as plt\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize']=10,20\n",
    "\n",
    "# Add the grandparent directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../')))\n",
    "from Utils import FE_helper as FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fc8d45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the data\n",
    "train_df = pd.read_csv('../Original_Data/train_2025.csv') \n",
    "test_df = pd.read_csv('../Original_Data/test_2025.csv')\n",
    "\n",
    "train_df = FE.add_features(train_df)\n",
    "test_df = FE.add_features(test_df)\n",
    "\n",
    "test_id = test_df['claim_number']\n",
    "train_id = train_df['claim_number']\n",
    "target = train_df['fraud']\n",
    "\n",
    "ignore_var = ['claim_date.is_weekend', 'claim_date.near_holiday', 'fraud']\n",
    "train_df = FE.drop_ignored_columns(train_df, ignore_var)\n",
    "test_df = FE.drop_ignored_columns(test_df, ignore_var)\n",
    "\n",
    "presence_info_df_3 = pd.read_csv('logs/subset_info_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3466b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_presence_columns(train_df, presence_info_df, verbose = False):\n",
    "    \"\"\"\n",
    "    For each combo in presence_info_df, create a presence feature on train_df.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_df (pd.DataFrame): The training dataset.\n",
    "    - presence_info_df (pd.DataFrame): DataFrame containing 'feature' column\n",
    "      with names like 'feature1__feature2__feature3_present'.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: train_df with new presence columns added.\n",
    "    \"\"\"\n",
    "    df_out = train_df.copy()\n",
    "    \n",
    "    for combo_str in presence_info_df['feature']:\n",
    "        # Extract base combo name (strip trailing '_present')\n",
    "        if combo_str.endswith('_present'):\n",
    "            combo_base = combo_str[:-8]\n",
    "        else:\n",
    "            combo_base = combo_str\n",
    "        \n",
    "        # Split by '__' to get the individual features\n",
    "        combo_features = combo_base.split('__')\n",
    "        new_col_name = combo_base + '_present'  # keep consistent\n",
    "        if verbose:\n",
    "            print(f\"Processing combo: {combo_features}\")\n",
    "        \n",
    "        # Build tuple of feature values per row\n",
    "        combo_tuples = train_df[combo_features].apply(tuple, axis=1)\n",
    "        \n",
    "        # Count how many times each tuple appears\n",
    "        counts = combo_tuples.map(combo_tuples.value_counts())\n",
    "        \n",
    "        # Presence = appears more than once in the dataset\n",
    "        df_out[new_col_name] = (counts > 1).astype(int)\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "\n",
    "def fit_presence_pca(train_df, present_cols, n_components=None, scale=True):\n",
    "    \"\"\"\n",
    "    Fits PCA on presence feature columns in the training set.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_df (pd.DataFrame): Training dataset.\n",
    "    - present_cols (list): List of column names for presence features.\n",
    "    - n_components (int or None): Number of PCA components. If None, keep all.\n",
    "    - scale (bool): Whether to standardize columns before PCA.\n",
    "    \n",
    "    Returns:\n",
    "    - pca (PCA object): Fitted PCA object.\n",
    "    - X_train_pca (np.ndarray): Transformed training set (PCA scores).\n",
    "    - scaler (StandardScaler object or None): Fitted scaler if used, else None.\n",
    "    \"\"\"\n",
    "    X = train_df[present_cols].values\n",
    "\n",
    "    # Optionally scale features\n",
    "    scaler = None\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "    # Fit PCA\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    X_train_pca = pca.fit_transform(X)\n",
    "\n",
    "    print(f\"PCA fitted. Explained variance (first 10 components): {pca.explained_variance_ratio_[:10]}\")\n",
    "    return pca, X_train_pca, scaler\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "def fit_regular_transformer(train_df, presence_suffix='_present'):\n",
    "    # Identify regular columns\n",
    "    regular_cols = [col for col in train_df.columns if not col.endswith(presence_suffix)]\n",
    "    \n",
    "    # Split regular into categorical and numerical\n",
    "    categorical_cols = train_df[regular_cols].select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_cols = train_df[regular_cols].select_dtypes(include=['number']).columns.tolist()\n",
    "    if 'claim_number' in numerical_cols:\n",
    "        numerical_cols.remove('claim_number')\n",
    "    \n",
    "    # Initialize transformers\n",
    "    onehot = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit transformers\n",
    "    onehot.fit(train_df[categorical_cols])\n",
    "    scaler.fit(train_df[numerical_cols])\n",
    "    \n",
    "    # print(f\"Fitted on {len(categorical_cols)} categorical and {len(numerical_cols)} numerical columns.\")\n",
    "    \n",
    "    return onehot, scaler, categorical_cols, numerical_cols\n",
    "\n",
    "def transform_regular_set(df, onehot, scaler, categorical_cols, numerical_cols):\n",
    "    # Transform categorical\n",
    "    cat_transformed = onehot.transform(df[categorical_cols])\n",
    "    cat_df = pd.DataFrame(cat_transformed, columns=onehot.get_feature_names_out(categorical_cols), index=df.index)\n",
    "    \n",
    "    # Transform numerical\n",
    "    num_transformed = scaler.transform(df[numerical_cols])\n",
    "    num_df = pd.DataFrame(num_transformed, columns=numerical_cols, index=df.index)\n",
    "    \n",
    "    # Combine transformed parts\n",
    "    transformed_df = pd.concat([num_df, cat_df], axis=1)\n",
    "    \n",
    "    # print(f\"Transformed set shape: {transformed_df.shape}\")\n",
    "    return transformed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb3363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, train_df, presence_info_df, target, kfoldcv=5, drop=[]):\n",
    "    # Dynamic feature selection thresholds\n",
    "    difference_min = trial.suggest_float('difference_min', 0.03, 0.07)\n",
    "    info_min = trial.suggest_float('info_min', 0.03, 0.05)\n",
    "    presence_info_df = presence_info_df[\n",
    "        (np.abs(presence_info_df['difference']) > difference_min) &\n",
    "        (presence_info_df['info'] > info_min)\n",
    "    ]\n",
    "\n",
    "    trial.set_user_attr('difference_min', difference_min)\n",
    "    trial.set_user_attr('info_min', info_min)\n",
    "\n",
    "    # Add presence features\n",
    "    updated_train_df = add_presence_columns(train_df, presence_info_df)\n",
    "    present_cols = [col for col in updated_train_df.columns if col.endswith('_present')]\n",
    "\n",
    "    # Drop problematic high-cardinality categorical columns\n",
    "    high_dim_cat_cols_to_drop = ['claim_date.day', 'claim_date.dayofweek', 'claim_date.weekofyear',\n",
    "    'claim_date.month', 'zero_payout', 'zero_payout']\n",
    "    updated_train_df.drop(columns=high_dim_cat_cols_to_drop, inplace=True)\n",
    "\n",
    "    # Transform regular features\n",
    "    onehot, scaler, cat_cols, num_cols = fit_regular_transformer(updated_train_df)\n",
    "    X_train_regular = transform_regular_set(updated_train_df, onehot, scaler, cat_cols, num_cols)\n",
    "\n",
    "    # Combine with presence features\n",
    "    full_train_df = pd.concat([X_train_regular, updated_train_df[present_cols]], axis=1)\n",
    "\n",
    "    # Define hyperparameter space\n",
    "    params = {\n",
    "        'iterations': 2000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),\n",
    "        'depth': trial.suggest_int('depth', 3, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 25.0, log=True),\n",
    "        'random_seed': 69,\n",
    "        'loss_function': 'Logloss',\n",
    "        'eval_metric': 'Logloss',\n",
    "        'verbose': False,\n",
    "        'task_type': 'GPU'  # change to 'GPU' if using GPU\n",
    "    }\n",
    "\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=kfoldcv, shuffle=True, random_state=42)\n",
    "    best_thresholds = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(full_train_df, target):\n",
    "        X_train = full_train_df.iloc[train_idx].drop(columns=drop, errors='ignore')\n",
    "        X_val = full_train_df.iloc[val_idx].drop(columns=drop, errors='ignore')\n",
    "        y_train = target.iloc[train_idx]\n",
    "        y_val = target.iloc[val_idx]\n",
    "        \n",
    "        train_pool = Pool(X_train, y_train)\n",
    "        val_pool = Pool(X_val, y_val)\n",
    "\n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=50)\n",
    "\n",
    "        # Predict + threshold tuning\n",
    "        probs = model.predict_proba(X_val)[:, 1]\n",
    "        thresholds = np.linspace(0.1, 0.9, 50)\n",
    "        f1s = [f1_score(y_val, probs > t) for t in thresholds]\n",
    "\n",
    "        best_f1 = max(f1s)\n",
    "        best_threshold = thresholds[np.argmax(f1s)]\n",
    "\n",
    "        f1_scores.append(best_f1)\n",
    "        best_thresholds.append(best_threshold)\n",
    "\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    mean_threshold = np.mean(best_thresholds)\n",
    "\n",
    "    trial.set_user_attr('mean_threshold', mean_threshold)\n",
    "    trial.set_user_attr('f1_per_fold', f1_scores)\n",
    "\n",
    "    return mean_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "900064c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 21:56:40,546] A new study created in memory with name: no-name-0d9a15b3-e194-4f89-89b3-dc588032b73b\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "[I 2025-05-06 21:57:51,653] Trial 0 finished with value: 0.37297727498480876 and parameters: {'difference_min': 0.05314297903783603, 'info_min': 0.0406597816513935, 'learning_rate': 0.018617826835175474, 'depth': 3, 'l2_leaf_reg': 5.1348291130348676}. Best is trial 0 with value: 0.37297727498480876.\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "[I 2025-05-06 21:58:28,104] Trial 1 finished with value: 0.3742495761710329 and parameters: {'difference_min': 0.052992952107804595, 'info_min': 0.032072896590969506, 'learning_rate': 0.05977688345217947, 'depth': 5, 'l2_leaf_reg': 1.050431830374738}. Best is trial 1 with value: 0.3742495761710329.\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "[I 2025-05-06 21:58:57,466] Trial 2 finished with value: 0.36859552747538593 and parameters: {'difference_min': 0.04092226593371923, 'info_min': 0.045705021598209536, 'learning_rate': 0.04471763175244346, 'depth': 8, 'l2_leaf_reg': 0.00524354581116958}. Best is trial 1 with value: 0.3742495761710329.\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "[I 2025-05-06 21:59:14,074] Trial 3 finished with value: 0.3692357314638304 and parameters: {'difference_min': 0.04557139128447096, 'info_min': 0.041801928834469634, 'learning_rate': 0.0990052058788047, 'depth': 6, 'l2_leaf_reg': 0.002119702782318182}. Best is trial 1 with value: 0.3742495761710329.\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "[W 2025-05-06 21:59:40,085] Trial 4 failed with parameters: {'difference_min': 0.06827326053143944, 'info_min': 0.03453463113914386, 'learning_rate': 0.011594052011639072, 'depth': 6, 'l2_leaf_reg': 0.018697542386167144} because of the following error: KeyboardInterrupt('').\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yangs\\AppData\\Local\\Temp\\ipykernel_30780\\2002147916.py\", line 5, in <lambda>\n",
      "    study.optimize(lambda trial: objective(trial, train_df, presence_info_df_3, target, kfoldcv= 5),\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yangs\\AppData\\Local\\Temp\\ipykernel_30780\\3701994433.py\", line 57, in objective\n",
      "    model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=50)\n",
      "  File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\catboost\\core.py\", line 5245, in fit\n",
      "    self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,\n",
      "  File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\catboost\\core.py\", line 2410, in _fit\n",
      "    self._train(\n",
      "  File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\catboost\\core.py\", line 1790, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 5023, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 5072, in _catboost._CatBoost._train\n",
      "KeyboardInterrupt\n",
      "[W 2025-05-06 21:59:40,086] Trial 4 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m pruner \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mpruners\u001b[38;5;241m.\u001b[39mMedianPruner()\n\u001b[0;32m      4\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m, pruner\u001b[38;5;241m=\u001b[39mpruner)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_info_df_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkfoldcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m350\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m best_f1 \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_value\n",
      "File \u001b[1;32mc:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m      3\u001b[0m pruner \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mpruners\u001b[38;5;241m.\u001b[39mMedianPruner()\n\u001b[0;32m      4\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m, pruner\u001b[38;5;241m=\u001b[39mpruner)\n\u001b[1;32m----> 5\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_info_df_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkfoldcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m      6\u001b[0m                 n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m350\u001b[39m)\n\u001b[0;32m      9\u001b[0m best_f1 \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_value\n",
      "Cell \u001b[1;32mIn[22], line 57\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial, train_df, presence_info_df, target, kfoldcv, drop)\u001b[0m\n\u001b[0;32m     54\u001b[0m val_pool \u001b[38;5;241m=\u001b[39m Pool(X_val, y_val)\n\u001b[0;32m     56\u001b[0m model \u001b[38;5;241m=\u001b[39m CatBoostClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m---> 57\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Predict + threshold tuning\u001b[39;00m\n\u001b[0;32m     60\u001b[0m probs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_val)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\catboost\\core.py:5245\u001b[0m, in \u001b[0;36mCatBoostClassifier.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m   5243\u001b[0m     CatBoostClassifier\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m-> 5245\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5246\u001b[0m \u001b[43m          \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5247\u001b[0m \u001b[43m          \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\catboost\\core.py:2410\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2407\u001b[0m allow_clear_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_clear_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2409\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining plots\u001b[39m\u001b[38;5;124m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params())]):\n\u001b[1;32m-> 2410\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_sets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minit_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2418\u001b[0m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[0;32m   2419\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[1;32mc:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\catboost\\core.py:1790\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[1;32m-> 1790\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[1;32m_catboost.pyx:5023\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:5072\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "presence_info_df_3 = pd.read_csv('logs/subset_info_3.csv')\n",
    "# Run Optuna\n",
    "pruner = optuna.pruners.MedianPruner()\n",
    "study = optuna.create_study(direction='maximize', pruner=pruner)\n",
    "study.optimize(lambda trial: objective(trial, train_df, presence_info_df_3, target, kfoldcv= 5),\n",
    "                n_trials=350)\n",
    "\n",
    "\n",
    "best_f1 = study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd45e6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1:\n",
      "  Value: 0.3818334954271173\n",
      "  Params: {'difference_min': 0.05218960132104022, 'info_min': 0.03089648066623365, 'max_depth': 3, 'min_child_weight': 0.01436830166963303, 'subsample': 0.5535868944752692, 'colsample_bytree': 0.7846012921773652, 'learning_rate': 0.06806594650165576, 'lambda': 3.7018701051348804, 'alpha': 6.572640433297191}\n",
      "Top 2:\n",
      "  Value: 0.38176523111697397\n",
      "  Params: {'difference_min': 0.05312839082120633, 'info_min': 0.030736703106280343, 'max_depth': 3, 'min_child_weight': 0.32370419594904, 'subsample': 0.59928822224624, 'colsample_bytree': 0.7678473689425825, 'learning_rate': 0.05211631620033941, 'lambda': 3.586250039728482, 'alpha': 10.884899562909776}\n",
      "Top 3:\n",
      "  Value: 0.3811044865717462\n",
      "  Params: {'difference_min': 0.051872682321035646, 'info_min': 0.03099618801080478, 'max_depth': 3, 'min_child_weight': 0.17417128165061507, 'subsample': 0.6350551258507653, 'colsample_bytree': 0.8309610101172697, 'learning_rate': 0.06606518113744322, 'lambda': 0.6177716658941924, 'alpha': 9.916371573954958}\n",
      "Top 4:\n",
      "  Value: 0.3809434788887788\n",
      "  Params: {'difference_min': 0.0520001355680931, 'info_min': 0.03220079756430095, 'max_depth': 3, 'min_child_weight': 0.32300344932921493, 'subsample': 0.6627983881358241, 'colsample_bytree': 0.8117998112931287, 'learning_rate': 0.06033626086706421, 'lambda': 0.6160816434582351, 'alpha': 10.829932539412924}\n",
      "Top 5:\n",
      "  Value: 0.38079372030606773\n",
      "  Params: {'difference_min': 0.052923469830675945, 'info_min': 0.03179163884510338, 'max_depth': 3, 'min_child_weight': 0.2284806429190546, 'subsample': 0.5884600097447273, 'colsample_bytree': 0.8016134502319052, 'learning_rate': 0.0694064863006163, 'lambda': 2.6716785647962724, 'alpha': 8.403363899744898}\n"
     ]
    }
   ],
   "source": [
    "top_trials = sorted(study.trials, key=lambda t: t.value, reverse=True)[:5]  # For minimization\n",
    "# or reverse=True if you're maximizing\n",
    "\n",
    "# Print parameters of top trials\n",
    "for i, trial in enumerate(top_trials, 1):\n",
    "    print(f\"Top {i}:\")\n",
    "    print(f\"  Value: {trial.value}\")\n",
    "    print(f\"  Params: {trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b7a6689",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_min, info_min = study.best_trial.user_attrs['difference_min'], study.best_trial.user_attrs['info_min']\n",
    "\n",
    "presence_info_df_3 = presence_info_df_3[(np.abs(presence_info_df_3['difference']) > difference_min) & (presence_info_df_3['info'] > info_min)]\n",
    "presence_info_df = presence_info_df_3\n",
    "\n",
    "updated_train_df = add_presence_columns(train_df, presence_info_df)\n",
    "updated_test_df = add_presence_columns(test_df, presence_info_df)\n",
    "\n",
    "present_cols = [col for col in updated_train_df.columns if col.endswith('_present')]\n",
    "# print(updated_train_df[non_present_cols].info())\n",
    "\n",
    "\n",
    "high_dim_cat_cols_to_drop = ['claim_date.day', 'claim_date.dayofweek', 'claim_date.weekofyear', 'claim_date.month', 'zero_payout']\n",
    "updated_train_df.drop(columns = high_dim_cat_cols_to_drop, inplace=True)\n",
    "updated_test_df.drop(columns = high_dim_cat_cols_to_drop, inplace=True)\n",
    "\n",
    "# Step 1: Fit on training data\n",
    "onehot, scaler, cat_cols, num_cols = fit_regular_transformer(updated_train_df)\n",
    "\n",
    "# Step 2: Transform training set itself\n",
    "X_train_regular = transform_regular_set(updated_train_df, onehot, scaler, cat_cols, num_cols)\n",
    "\n",
    "# Step 3: Transform test set (call the same function on test_df)\n",
    "X_test_regular = transform_regular_set(test_df, onehot, scaler, cat_cols, num_cols)\n",
    "\n",
    "# present_cols = []\n",
    "# Combine for train\n",
    "updated_train_final = pd.concat([X_train_regular, updated_train_df[present_cols]], axis=1)\n",
    "\n",
    "# Combine for test\n",
    "updated_test_final = pd.concat([X_test_regular, updated_test_df[present_cols]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d823465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train DataFrame to ../Records/xgb_temp\\train_2025.csv\n",
      "Saved test DataFrame to ../Records/xgb_temp\\test_2025.csv\n",
      "Saved best parameters to ../Records/xgb_temp\\param_xgb_temp.json\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "import json\n",
    "output_dir = '../Records/xgb_temp'\n",
    "train_csv_path = os.path.join(output_dir, 'train_2025.csv')\n",
    "test_csv_path = os.path.join(output_dir, 'test_2025.csv')\n",
    "param_json_path = os.path.join(output_dir, 'param_xgb_temp.json')\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save DataFrames\n",
    "train_df.to_csv(train_csv_path, index=False)\n",
    "print(f\"Saved train DataFrame to {train_csv_path}\")\n",
    "\n",
    "test_df.to_csv(test_csv_path, index=False)\n",
    "print(f\"Saved test DataFrame to {test_csv_path}\")\n",
    "\n",
    "# Save best_params as JSON\n",
    "best_threshold = study.best_trial.user_attrs['mean_threshold']\n",
    "best_params = study.best_params\n",
    "best_params.update({'mean_threshold': float(best_threshold)})\n",
    "with open(param_json_path, 'w') as f:\n",
    "    json.dump(best_params, f, indent=4)\n",
    "print(f\"Saved best parameters to {param_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "205ca36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-auc:0.72686\n",
      "[100]\tvalidation-auc:0.71828\n",
      "[136]\tvalidation-auc:0.69079\n"
     ]
    }
   ],
   "source": [
    "# Split train into final train and validation sets\n",
    "X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(\n",
    "    updated_train_final, target, test_size=0.01, stratify=target, random_state=42\n",
    ")\n",
    "\n",
    "# Clean best_params: keep only what XGBoost expects\n",
    "xgb_param_keys = [\n",
    "    'objective', 'eval_metric', 'tree_method', 'max_depth', 'min_child_weight',\n",
    "    'subsample', 'colsample_bytree', 'eta', 'lambda', 'alpha', 'seed', 'verbosity'\n",
    "]\n",
    "clean_params = {k: best_params[k] for k in xgb_param_keys if k in best_params}\n",
    "\n",
    "# Set required XGBoost parameters\n",
    "clean_params.update({\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': ['auc'],  # or just 'auc'\n",
    "    'tree_method': 'hist',  # or 'gpu_hist' if using GPU\n",
    "    'seed': 42,\n",
    "    'verbosity': 0  # silent mode\n",
    "})\n",
    "\n",
    "# Convert to DMatrix\n",
    "dtrain = xgb.DMatrix(X_train_final, label=y_train_final)\n",
    "dval = xgb.DMatrix(X_val_final, label=y_val_final)\n",
    "dtest = xgb.DMatrix(updated_test_final)\n",
    "\n",
    "# Train with early stopping\n",
    "final_model = xgb.train(\n",
    "    clean_params,\n",
    "    dtrain,\n",
    "    num_boost_round=5000,\n",
    "    evals=[(dval, 'validation')],\n",
    "    early_stopping_rounds=100,\n",
    "    verbose_eval=100  # adjust or set to False for silence\n",
    ")\n",
    "\n",
    "# Predict on test set (probabilities)\n",
    "probs_test = final_model.predict(dtest, iteration_range=(0, final_model.best_iteration + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3dfa6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%m%d_%H%M')\n",
    "\n",
    "final_preds = (probs_test > best_threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'claim_number': test_id,\n",
    "    'fraud': final_preds\n",
    "})\n",
    "\n",
    "submission.to_csv(f'../Submit/submission_xgb.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
