{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfe1b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sys\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pylab as plt\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize']=10,20\n",
    "\n",
    "# Add the grandparent directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../')))\n",
    "from Utils import FE_helper as FE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fc8d45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the data\n",
    "train_df = pd.read_csv('../Original_Data/train_2025.csv') \n",
    "test_df = pd.read_csv('../Original_Data/test_2025.csv')\n",
    "\n",
    "train_df = FE.add_features(train_df)\n",
    "test_df = FE.add_features(test_df)\n",
    "\n",
    "test_id = test_df['claim_number']\n",
    "train_id = train_df['claim_number']\n",
    "target = train_df['fraud']\n",
    "\n",
    "ignore_var = ['claim_date.is_weekend', 'claim_date.near_holiday', 'fraud']\n",
    "train_df = FE.drop_ignored_columns(train_df, ignore_var)\n",
    "test_df = FE.drop_ignored_columns(test_df, ignore_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30951fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# presence_info_df_2 = pd.read_csv('logs/subset_info_2.csv')\n",
    "# presence_info_df_2 = presence_info_df_2[(np.abs(presence_info_df_2['difference']) > 0.065) | (presence_info_df_2['info'] > 0.045)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9819f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# presence_info_df = pd.concat([presence_info_df_2, presence_info_df_3])\n",
    "presence_info_df_3 = pd.read_csv('logs/subset_info_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3466b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_presence_columns(train_df, presence_info_df, verbose = False):\n",
    "    \"\"\"\n",
    "    For each combo in presence_info_df, create a presence feature on train_df.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_df (pd.DataFrame): The training dataset.\n",
    "    - presence_info_df (pd.DataFrame): DataFrame containing 'feature' column\n",
    "      with names like 'feature1__feature2__feature3_present'.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: train_df with new presence columns added.\n",
    "    \"\"\"\n",
    "    df_out = train_df.copy()\n",
    "    \n",
    "    for combo_str in presence_info_df['feature']:\n",
    "        # Extract base combo name (strip trailing '_present')\n",
    "        if combo_str.endswith('_present'):\n",
    "            combo_base = combo_str[:-8]\n",
    "        else:\n",
    "            combo_base = combo_str\n",
    "        \n",
    "        # Split by '__' to get the individual features\n",
    "        combo_features = combo_base.split('__')\n",
    "        new_col_name = combo_base + '_present'  # keep consistent\n",
    "        if verbose:\n",
    "            print(f\"Processing combo: {combo_features}\")\n",
    "        \n",
    "        # Build tuple of feature values per row\n",
    "        combo_tuples = train_df[combo_features].apply(tuple, axis=1)\n",
    "        \n",
    "        # Count how many times each tuple appears\n",
    "        counts = combo_tuples.map(combo_tuples.value_counts())\n",
    "        \n",
    "        # Presence = appears more than once in the dataset\n",
    "        df_out[new_col_name] = (counts > 1).astype(int)\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "\n",
    "def fit_presence_pca(train_df, present_cols, n_components=None, scale=True):\n",
    "    \"\"\"\n",
    "    Fits PCA on presence feature columns in the training set.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_df (pd.DataFrame): Training dataset.\n",
    "    - present_cols (list): List of column names for presence features.\n",
    "    - n_components (int or None): Number of PCA components. If None, keep all.\n",
    "    - scale (bool): Whether to standardize columns before PCA.\n",
    "    \n",
    "    Returns:\n",
    "    - pca (PCA object): Fitted PCA object.\n",
    "    - X_train_pca (np.ndarray): Transformed training set (PCA scores).\n",
    "    - scaler (StandardScaler object or None): Fitted scaler if used, else None.\n",
    "    \"\"\"\n",
    "    X = train_df[present_cols].values\n",
    "\n",
    "    # Optionally scale features\n",
    "    scaler = None\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "    # Fit PCA\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    X_train_pca = pca.fit_transform(X)\n",
    "\n",
    "    print(f\"PCA fitted. Explained variance (first 10 components): {pca.explained_variance_ratio_[:10]}\")\n",
    "    return pca, X_train_pca, scaler\n",
    "\n",
    "\n",
    "\n",
    "def fit_regular_transformer(train_df, presence_suffix='_present'):\n",
    "    # Identify regular columns\n",
    "    regular_cols = [col for col in train_df.columns if not col.endswith(presence_suffix)]\n",
    "    \n",
    "    # Split regular into categorical and numerical\n",
    "    categorical_cols = train_df[regular_cols].select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_cols = train_df[regular_cols].select_dtypes(include=['number']).columns.tolist()\n",
    "    if 'claim_number' in numerical_cols:\n",
    "        numerical_cols.remove('claim_number')\n",
    "    \n",
    "    # Initialize transformers\n",
    "    onehot = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit transformers\n",
    "    onehot.fit(train_df[categorical_cols])\n",
    "    scaler.fit(train_df[numerical_cols])\n",
    "    \n",
    "    # print(f\"Fitted on {len(categorical_cols)} categorical and {len(numerical_cols)} numerical columns.\")\n",
    "    \n",
    "    return onehot, scaler, categorical_cols, numerical_cols\n",
    "\n",
    "def transform_regular_set(df, onehot, scaler, categorical_cols, numerical_cols):\n",
    "    # Transform categorical\n",
    "    cat_transformed = onehot.transform(df[categorical_cols])\n",
    "    cat_df = pd.DataFrame(cat_transformed, columns=onehot.get_feature_names_out(categorical_cols), index=df.index)\n",
    "    \n",
    "    # Transform numerical\n",
    "    num_transformed = scaler.transform(df[numerical_cols])\n",
    "    num_df = pd.DataFrame(num_transformed, columns=numerical_cols, index=df.index)\n",
    "    \n",
    "    # Combine transformed parts\n",
    "    transformed_df = pd.concat([num_df, cat_df], axis=1)\n",
    "    \n",
    "    # print(f\"Transformed set shape: {transformed_df.shape}\")\n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7400028",
   "metadata": {},
   "outputs": [],
   "source": [
    "presence_info_df_3 = presence_info_df_3[(np.abs(presence_info_df_3['difference']) > 0.05) & (presence_info_df_3['info'] > 0.03)]\n",
    "presence_info_df = presence_info_df_3\n",
    "\n",
    "updated_train_df = add_presence_columns(train_df, presence_info_df)\n",
    "updated_test_df = add_presence_columns(test_df, presence_info_df)\n",
    "\n",
    "present_cols = [col for col in updated_train_df.columns if col.endswith('_present')]\n",
    "non_present_cols = [col for col in updated_train_df.columns if not col.endswith('_present')]\n",
    "# print(updated_train_df[non_present_cols].info())\n",
    "\n",
    "\n",
    "high_dim_cat_cols_to_drop = ['claim_date.day', 'claim_date.dayofweek', 'claim_date.weekofyear', 'claim_date.month', 'zero_payout']\n",
    "updated_train_df.drop(columns = high_dim_cat_cols_to_drop, inplace=True)\n",
    "updated_test_df.drop(columns = high_dim_cat_cols_to_drop, inplace=True)\n",
    "\n",
    "# Step 1: Fit on training data\n",
    "onehot, scaler, cat_cols, num_cols = fit_regular_transformer(updated_train_df)\n",
    "\n",
    "# Step 2: Transform training set itself\n",
    "X_train_regular = transform_regular_set(updated_train_df, onehot, scaler, cat_cols, num_cols)\n",
    "\n",
    "# Step 3: Transform test set (call the same function on test_df)\n",
    "X_test_regular = transform_regular_set(test_df, onehot, scaler, cat_cols, num_cols)\n",
    "\n",
    "# present_cols = []\n",
    "# Combine for train\n",
    "full_train_df = pd.concat([X_train_regular, updated_train_df[present_cols]], axis=1)\n",
    "\n",
    "# Combine for test\n",
    "updated_test_final = pd.concat([X_test_regular, updated_test_df[present_cols]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69fb3363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, train_df, presence_info_df, target, kfoldcv=5, drop=[]):\n",
    "    difference_min = trial.suggest_float('difference_min', 0.03, 0.07)\n",
    "    info_min = trial.suggest_float('info_min', 0.03, 0.05)\n",
    "    presence_info_df = presence_info_df[(np.abs(presence_info_df['difference']) > difference_min) \n",
    "                                        & (presence_info_df['info'] > info_min)]\n",
    "\n",
    "    # Save the used values to trial attributes\n",
    "    trial.set_user_attr('difference_min', difference_min)\n",
    "    trial.set_user_attr('info_min', info_min)\n",
    "    \n",
    "    \n",
    "    updated_train_df = add_presence_columns(train_df, presence_info_df)\n",
    "    present_cols = [col for col in updated_train_df.columns if col.endswith('_present')]\n",
    "    \n",
    "    high_dim_cat_cols_to_drop = ['claim_date.day', 'claim_date.dayofweek', 'claim_date.weekofyear', 'claim_date.month', 'zero_payout']\n",
    "    updated_train_df.drop(columns = high_dim_cat_cols_to_drop, inplace=True)\n",
    "    \n",
    "    # Step 1: Fit on training data\n",
    "    onehot, scaler, cat_cols, num_cols = fit_regular_transformer(updated_train_df)\n",
    "\n",
    "    # Step 2: Transform training set itself\n",
    "    X_train_regular = transform_regular_set(updated_train_df, onehot, scaler, cat_cols, num_cols)\n",
    "    \n",
    "    full_train_df = pd.concat([X_train_regular, updated_train_df[present_cols]], axis=1)\n",
    "    \n",
    "    # Define hyperparameter space\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'is_unbalance': True,\n",
    "        'boosting': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 50, 90),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.3, 0.6),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.3, 0.9),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 5, 25),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-3, 15.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-3, 10.0, log=True),\n",
    "        'verbose': -1,\n",
    "        'seed':69\n",
    "    }\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=kfoldcv, shuffle=True, random_state=42)\n",
    "    best_thresholds = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(full_train_df, target):\n",
    "        X_train = full_train_df.iloc[train_idx].drop(columns=drop, errors='ignore')\n",
    "        X_val = full_train_df.iloc[val_idx].drop(columns=drop, errors='ignore')\n",
    "        y_train = target.iloc[train_idx]\n",
    "        y_val = target.iloc[val_idx]\n",
    "\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    "\n",
    "        model = lgb.train(\n",
    "                params,\n",
    "                lgb_train,\n",
    "                num_boost_round=2000,\n",
    "                valid_sets=[lgb_val],\n",
    "                callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    "            )\n",
    "\n",
    "        # Predict + threshold tuning\n",
    "        probs = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "        thresholds = np.linspace(0.1, 0.9, 50)\n",
    "        f1s = [f1_score(y_val, probs > t) for t in thresholds]\n",
    "\n",
    "        best_f1 = max(f1s)\n",
    "        best_threshold = thresholds[np.argmax(f1s)]\n",
    "\n",
    "        f1_scores.append(best_f1)\n",
    "        best_thresholds.append(best_threshold)\n",
    "\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    mean_threshold = np.mean(best_thresholds)\n",
    "\n",
    "    trial.set_user_attr('mean_threshold', mean_threshold)\n",
    "    trial.set_user_attr('f1_per_fold', f1_scores)\n",
    "\n",
    "    return mean_f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900064c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 17:24:33,797] A new study created in memory with name: no-name-ea0e9a5e-0974-4bb6-972f-06ec9e62244e\n",
      "[I 2025-05-06 17:24:37,140] Trial 0 finished with value: 0.35199343888565016 and parameters: {'difference_min': 0.06471545020482582, 'info_min': 0.045868762423383364, 'num_leaves': 68, 'feature_fraction': 0.5219578322607082, 'bagging_fraction': 0.6540210400617349, 'bagging_freq': 7, 'learning_rate': 0.08561267579841303, 'lambda_l1': 0.008094914939753206, 'lambda_l2': 0.031769622472856564}. Best is trial 0 with value: 0.35199343888565016.\n",
      "[I 2025-05-06 17:24:45,024] Trial 1 finished with value: 0.355756128984545 and parameters: {'difference_min': 0.041947822964233175, 'info_min': 0.033816176067246066, 'num_leaves': 50, 'feature_fraction': 0.4129123933609268, 'bagging_fraction': 0.36842158891724724, 'bagging_freq': 21, 'learning_rate': 0.08491405800875192, 'lambda_l1': 3.9322996508670367, 'lambda_l2': 1.3067227907635852}. Best is trial 1 with value: 0.355756128984545.\n",
      "[I 2025-05-06 17:24:49,910] Trial 2 finished with value: 0.3620778307108854 and parameters: {'difference_min': 0.05526891538020838, 'info_min': 0.04850333376004608, 'num_leaves': 88, 'feature_fraction': 0.4132345714335572, 'bagging_fraction': 0.8929701614025287, 'bagging_freq': 11, 'learning_rate': 0.0596271206267281, 'lambda_l1': 0.0052676916981049425, 'lambda_l2': 0.005605611509147523}. Best is trial 2 with value: 0.3620778307108854.\n",
      "[I 2025-05-06 17:24:54,903] Trial 3 finished with value: 0.35816811503133567 and parameters: {'difference_min': 0.03265335067670039, 'info_min': 0.040023068754280855, 'num_leaves': 58, 'feature_fraction': 0.38094450047471173, 'bagging_fraction': 0.4796864710196882, 'bagging_freq': 14, 'learning_rate': 0.07905818645175935, 'lambda_l1': 3.5074008776589167, 'lambda_l2': 0.07454678664913594}. Best is trial 2 with value: 0.3620778307108854.\n",
      "[I 2025-05-06 17:25:01,707] Trial 4 finished with value: 0.36373094865042643 and parameters: {'difference_min': 0.0639234051752828, 'info_min': 0.049861713255224496, 'num_leaves': 54, 'feature_fraction': 0.3147698623050232, 'bagging_fraction': 0.7007248711887231, 'bagging_freq': 17, 'learning_rate': 0.01885661626960136, 'lambda_l1': 0.008304395211934743, 'lambda_l2': 0.0010124909569010328}. Best is trial 4 with value: 0.36373094865042643.\n",
      "[I 2025-05-06 17:25:08,340] Trial 5 finished with value: 0.3652447804163713 and parameters: {'difference_min': 0.06308411193033928, 'info_min': 0.03057155267371397, 'num_leaves': 82, 'feature_fraction': 0.4096107068559761, 'bagging_fraction': 0.8289764065993899, 'bagging_freq': 16, 'learning_rate': 0.03370909046857904, 'lambda_l1': 0.014470481836620417, 'lambda_l2': 0.002143526240481965}. Best is trial 5 with value: 0.3652447804163713.\n",
      "[I 2025-05-06 17:25:12,240] Trial 6 finished with value: 0.3551928664059914 and parameters: {'difference_min': 0.05544603925203562, 'info_min': 0.035130081891754775, 'num_leaves': 66, 'feature_fraction': 0.3709952846409893, 'bagging_fraction': 0.8047923189288404, 'bagging_freq': 17, 'learning_rate': 0.09264513983956107, 'lambda_l1': 0.006929013794919948, 'lambda_l2': 0.42801616625195477}. Best is trial 5 with value: 0.3652447804163713.\n",
      "[I 2025-05-06 17:25:16,937] Trial 7 finished with value: 0.36069916466792545 and parameters: {'difference_min': 0.05285360830659595, 'info_min': 0.045477845879998695, 'num_leaves': 85, 'feature_fraction': 0.43483944460927315, 'bagging_fraction': 0.5447370722899084, 'bagging_freq': 12, 'learning_rate': 0.04382634368618305, 'lambda_l1': 3.1256034361602216, 'lambda_l2': 0.019185125457497662}. Best is trial 5 with value: 0.3652447804163713.\n",
      "[I 2025-05-06 17:25:20,842] Trial 8 finished with value: 0.3668629490033144 and parameters: {'difference_min': 0.056660610178709256, 'info_min': 0.04217338209032598, 'num_leaves': 80, 'feature_fraction': 0.5559089731752218, 'bagging_fraction': 0.4673594745092967, 'bagging_freq': 6, 'learning_rate': 0.07153487792561153, 'lambda_l1': 13.539348481728343, 'lambda_l2': 6.234023782283199}. Best is trial 8 with value: 0.3668629490033144.\n",
      "[I 2025-05-06 17:25:25,260] Trial 9 finished with value: 0.35592700439002467 and parameters: {'difference_min': 0.06576826277105725, 'info_min': 0.030461549264650004, 'num_leaves': 73, 'feature_fraction': 0.5693054615981299, 'bagging_fraction': 0.3043097579748344, 'bagging_freq': 20, 'learning_rate': 0.06420809414896683, 'lambda_l1': 6.451191529041499, 'lambda_l2': 0.1340265764120473}. Best is trial 8 with value: 0.3668629490033144.\n",
      "[I 2025-05-06 17:25:34,703] Trial 10 finished with value: 0.36388065591667934 and parameters: {'difference_min': 0.04507819903068608, 'info_min': 0.0407336114016944, 'num_leaves': 77, 'feature_fraction': 0.5085528576335535, 'bagging_fraction': 0.44244941583762887, 'bagging_freq': 6, 'learning_rate': 0.012849923309905233, 'lambda_l1': 0.3427644389553509, 'lambda_l2': 4.3224362274204955}. Best is trial 8 with value: 0.3668629490033144.\n",
      "[I 2025-05-06 17:25:39,699] Trial 11 finished with value: 0.36634764078249205 and parameters: {'difference_min': 0.060198871480042535, 'info_min': 0.036971746735697184, 'num_leaves': 81, 'feature_fraction': 0.4962735089067978, 'bagging_fraction': 0.7472263135640842, 'bagging_freq': 25, 'learning_rate': 0.03897128690893285, 'lambda_l1': 0.0866029713711239, 'lambda_l2': 7.592730369048544}. Best is trial 8 with value: 0.3668629490033144.\n",
      "[I 2025-05-06 17:25:45,094] Trial 12 finished with value: 0.35917333039659555 and parameters: {'difference_min': 0.05871903569725596, 'info_min': 0.03653061333131127, 'num_leaves': 79, 'feature_fraction': 0.5958938443302637, 'bagging_fraction': 0.7097303991602624, 'bagging_freq': 25, 'learning_rate': 0.037357891654983134, 'lambda_l1': 0.13851234837198723, 'lambda_l2': 8.172224343993669}. Best is trial 8 with value: 0.3668629490033144.\n",
      "[I 2025-05-06 17:25:49,249] Trial 13 finished with value: 0.35896695877352036 and parameters: {'difference_min': 0.047030917585000345, 'info_min': 0.042139939050321235, 'num_leaves': 89, 'feature_fraction': 0.5022450222910281, 'bagging_fraction': 0.576244563004144, 'bagging_freq': 25, 'learning_rate': 0.07065480522855955, 'lambda_l1': 0.62336720230493, 'lambda_l2': 1.8000920685921589}. Best is trial 8 with value: 0.3668629490033144.\n",
      "[I 2025-05-06 17:25:53,753] Trial 14 finished with value: 0.3582632115811787 and parameters: {'difference_min': 0.0687449997909297, 'info_min': 0.03719924457835524, 'num_leaves': 72, 'feature_fraction': 0.545050897570047, 'bagging_fraction': 0.48099376222926243, 'bagging_freq': 9, 'learning_rate': 0.0508423135115218, 'lambda_l1': 0.0634727243269286, 'lambda_l2': 0.486702805295553}. Best is trial 8 with value: 0.3668629490033144.\n",
      "[I 2025-05-06 17:26:00,399] Trial 15 finished with value: 0.3663973674970741 and parameters: {'difference_min': 0.05942668050634569, 'info_min': 0.04390723093462603, 'num_leaves': 77, 'feature_fraction': 0.4651833780355236, 'bagging_fraction': 0.7637986396436321, 'bagging_freq': 21, 'learning_rate': 0.024695416798666523, 'lambda_l1': 0.0011527768456776131, 'lambda_l2': 8.699979607718161}. Best is trial 8 with value: 0.3668629490033144.\n",
      "[I 2025-05-06 17:26:06,529] Trial 16 finished with value: 0.3610578772290757 and parameters: {'difference_min': 0.050207295428617564, 'info_min': 0.043987740767392834, 'num_leaves': 63, 'feature_fraction': 0.4624362406055953, 'bagging_fraction': 0.6266676865425039, 'bagging_freq': 22, 'learning_rate': 0.023764520102294556, 'lambda_l1': 0.0012423213467010505, 'lambda_l2': 2.01562062798024}. Best is trial 8 with value: 0.3668629490033144.\n",
      "[I 2025-05-06 17:26:19,109] Trial 17 finished with value: 0.369229224354702 and parameters: {'difference_min': 0.038317739171293184, 'info_min': 0.043179812809022756, 'num_leaves': 75, 'feature_fraction': 0.4664047485460663, 'bagging_fraction': 0.4067965711342547, 'bagging_freq': 5, 'learning_rate': 0.006309078699749542, 'lambda_l1': 0.0012696640017716546, 'lambda_l2': 0.4476768739593691}. Best is trial 17 with value: 0.369229224354702.\n",
      "[I 2025-05-06 17:26:37,805] Trial 18 finished with value: 0.36963753229569074 and parameters: {'difference_min': 0.036385879361123794, 'info_min': 0.04212743213850145, 'num_leaves': 73, 'feature_fraction': 0.5965222978269393, 'bagging_fraction': 0.37779084274967384, 'bagging_freq': 5, 'learning_rate': 0.006433858737074958, 'lambda_l1': 14.702242468653246, 'lambda_l2': 0.5538568306687973}. Best is trial 18 with value: 0.36963753229569074.\n",
      "[I 2025-05-06 17:26:46,689] Trial 19 finished with value: 0.3613556254504549 and parameters: {'difference_min': 0.035309776301111505, 'info_min': 0.04647284238012641, 'num_leaves': 62, 'feature_fraction': 0.31121558312000697, 'bagging_fraction': 0.37320604385124967, 'bagging_freq': 9, 'learning_rate': 0.007298587747363127, 'lambda_l1': 1.3016059038307781, 'lambda_l2': 0.5111021097407913}. Best is trial 18 with value: 0.36963753229569074.\n"
     ]
    }
   ],
   "source": [
    "presence_info_df_3 = pd.read_csv('logs/subset_info_3.csv')\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(lambda trial: objective(trial, train_df, presence_info_df_3, target, kfoldcv= 5),\n",
    "                n_trials=20)\n",
    "\n",
    "best_f1 = study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bca797a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_min, info_min = study.best_trial.user_attrs['difference_min'], study.best_trial.user_attrs['info_min']\n",
    "\n",
    "presence_info_df_3 = presence_info_df_3[(np.abs(presence_info_df_3['difference']) > difference_min) & (presence_info_df_3['info'] > info_min)]\n",
    "presence_info_df = presence_info_df_3\n",
    "\n",
    "updated_train_df = add_presence_columns(train_df, presence_info_df)\n",
    "updated_test_df = add_presence_columns(test_df, presence_info_df)\n",
    "\n",
    "present_cols = [col for col in updated_train_df.columns if col.endswith('_present')]\n",
    "# print(updated_train_df[non_present_cols].info())\n",
    "\n",
    "\n",
    "high_dim_cat_cols_to_drop = ['claim_date.day', 'claim_date.dayofweek', 'claim_date.weekofyear', 'claim_date.month', 'zero_payout']\n",
    "updated_train_df.drop(columns = high_dim_cat_cols_to_drop, inplace=True)\n",
    "updated_test_df.drop(columns = high_dim_cat_cols_to_drop, inplace=True)\n",
    "\n",
    "# Step 1: Fit on training data\n",
    "onehot, scaler, cat_cols, num_cols = fit_regular_transformer(updated_train_df)\n",
    "\n",
    "# Step 2: Transform training set itself\n",
    "X_train_regular = transform_regular_set(updated_train_df, onehot, scaler, cat_cols, num_cols)\n",
    "\n",
    "# Step 3: Transform test set (call the same function on test_df)\n",
    "X_test_regular = transform_regular_set(test_df, onehot, scaler, cat_cols, num_cols)\n",
    "\n",
    "# present_cols = []\n",
    "# Combine for train\n",
    "updated_train_final = pd.concat([X_train_regular, updated_train_df[present_cols]], axis=1)\n",
    "\n",
    "# Combine for test\n",
    "updated_test_final = pd.concat([X_test_regular, updated_test_df[present_cols]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5d823465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train DataFrame to ../Records/lgb_temp\\train_2025.csv\n",
      "Saved test DataFrame to ../Records/lgb_temp\\test_2025.csv\n",
      "Saved best parameters to ../Records/lgb_temp\\param_lgb_temp.json\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "import json\n",
    "output_dir = '../Records/lgb_temp'\n",
    "train_csv_path = os.path.join(output_dir, 'train_2025.csv')\n",
    "test_csv_path = os.path.join(output_dir, 'test_2025.csv')\n",
    "param_json_path = os.path.join(output_dir, 'param_lgb_temp.json')\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save DataFrames\n",
    "train_df.to_csv(train_csv_path, index=False)\n",
    "print(f\"Saved train DataFrame to {train_csv_path}\")\n",
    "\n",
    "test_df.to_csv(test_csv_path, index=False)\n",
    "print(f\"Saved test DataFrame to {test_csv_path}\")\n",
    "\n",
    "# Save best_params as JSON\n",
    "best_threshold = study.best_trial.user_attrs['mean_threshold']\n",
    "best_params = study.best_params\n",
    "best_params.update({'mean_threshold': float(best_threshold)})\n",
    "with open(param_json_path, 'w') as f:\n",
    "    json.dump(best_params, f, indent=4)\n",
    "print(f\"Saved best parameters to {param_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "205ca36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.687998\n",
      "[200]\tvalid_0's auc: 0.69252\n",
      "[300]\tvalid_0's auc: 0.695677\n",
      "[400]\tvalid_0's auc: 0.697212\n",
      "[500]\tvalid_0's auc: 0.697631\n",
      "Early stopping, best iteration is:\n",
      "[491]\tvalid_0's auc: 0.697965\n"
     ]
    }
   ],
   "source": [
    "# Split train into final train and validation sets\n",
    "X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(\n",
    "    updated_train_final, target, test_size=0.2, stratify=target, random_state=42\n",
    ")\n",
    "\n",
    "# Create LightGBM datasets\n",
    "lgb_train_final = lgb.Dataset(X_train_final, y_train_final)\n",
    "lgb_val_final = lgb.Dataset(X_val_final, y_val_final, reference=lgb_train_final)\n",
    "\n",
    "# Ensure best_params includes all needed keys\n",
    "best_params.update({\n",
    "    'objective': 'binary',\n",
    "    'metric': ['auc'],\n",
    "    'is_unbalance': True,\n",
    "    'boosting': 'gbdt',\n",
    "    'verbose': -1,\n",
    "    'seed': 42\n",
    "})\n",
    "\n",
    "# Train with early stopping\n",
    "final_model = lgb.train(\n",
    "    best_params,\n",
    "    lgb_train_final,\n",
    "    num_boost_round=5000,\n",
    "    valid_sets=[lgb_val_final],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=100),\n",
    "        lgb.log_evaluation(period=100)  # Optional: adjust or silence logging\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Predict on test set\n",
    "probs_test = final_model.predict(updated_test_final, num_iteration=final_model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3dfa6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%m%d_%H%M')\n",
    "\n",
    "final_preds = (probs_test > best_threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'claim_number': test_id,\n",
    "    'fraud': final_preds\n",
    "})\n",
    "\n",
    "submission.to_csv(f'../Submit/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
