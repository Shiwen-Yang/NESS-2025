{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "499c1ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 1. Import library\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2784c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", message=\".*load_learner.*insecure pickle.*\")\n",
    "\n",
    "# 2. Load the data\n",
    "train_df = pd.read_csv('../Data/processed/0427_01/train_2025.csv') \n",
    "test_df = pd.read_csv('../Data/processed/0427_01/test_2025.csv') \n",
    "\n",
    "train_df.drop(columns=\"claim_number\", inplace=True)\n",
    "test_id = test_df['claim_number']\n",
    "test_df.drop(columns=[\"claim_number\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401332eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250428_185003\"\n",
      "Preset alias specified: 'best' maps to 'best_quality'.\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.11.12\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          16\n",
      "Memory Avail:       15.34 GB / 31.93 GB (48.0%)\n",
      "Disk Space Avail:   285.19 GB / 935.97 GB (30.5%)\n",
      "===================================================\n",
      "Presets specified: ['best']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n",
      "2025-04-28 14:50:03,635\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-04-28 14:50:06,083\tINFO worker.py:1810 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"c:\\Users\\yangs\\Documents\\Python Projects\\NESS-2025\\Create_Tune_Models\\AutogluonModels\\ag-20250428_185003\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Beginning AutoGluon training ... Time limit = 896s\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m AutoGluon will save models to \"c:\\Users\\yangs\\Documents\\Python Projects\\NESS-2025\\Create_Tune_Models\\AutogluonModels\\ag-20250428_185003\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Train Data Rows:    16000\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Train Data Columns: 47\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Label Column:       fraud\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Problem Type:       binary\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tAvailable Memory:                    15048.15 MB\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tTrain Data (Original)  Memory Usage: 8.24 MB (0.1% of available memory)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t\t\tNote: Converting 16 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t\t('bool', [])   :  1 | ['relative_income_ind']\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t\t('float', [])  :  6 | ['marital_status', 'witness_present_ind', 'claim_est_payout', 'vehicle_price', 'vehicle_weight', ...]\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t\t('int', [])    : 37 | ['age_of_driver', 'gender', 'safty_rating', 'annual_income', 'high_education_ind', ...]\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t\t('object', []) :  3 | ['zipcode_type', 'state', 'county']\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t\t('category', [])  :  3 | ['zipcode_type', 'state', 'county']\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t\t('float', [])     :  6 | ['marital_status', 'witness_present_ind', 'claim_est_payout', 'vehicle_price', 'vehicle_weight', ...]\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t\t('int', [])       : 22 | ['age_of_driver', 'safty_rating', 'annual_income', 'zip_code', 'past_num_of_claims', ...]\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t\t('int', ['bool']) : 16 | ['gender', 'high_education_ind', 'address_change_ind', 'living_status', 'policy_report_filed_ind', ...]\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.2s = Fit runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t47 features in original data used to generate 47 features in processed data.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tTrain Data (Processed) Memory Usage: 3.71 MB (0.0% of available memory)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Data preprocessing and feature engineering runtime = 0.17s ...\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Excluded models: ['KNN', 'NN_TORCH'] (Specified by `excluded_model_types`)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting 87 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 596.98s of the 895.68s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.23%)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8431\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t1.78s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.08s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 590.27s of the 888.98s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.24%)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8425\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t1.38s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.06s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 585.81s of the 884.52s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8417\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t1.04s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.53s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 584.07s of the 882.78s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8417\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.94s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.44s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 582.53s of the 881.23s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.29%)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8427\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t32.95s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 546.72s of the 845.43s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8419\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.73s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.5s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 545.28s of the 843.98s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8417\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.69s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.56s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 543.81s of the 842.51s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.29%)\n",
      "\u001b[36m(_ray_fit pid=9632)\u001b[0m No improvement since epoch 3: early stopping\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8351\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t19.31s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.26s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 521.67s of the 820.38s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.35%)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8422\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t2.82s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.09s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 516.00s of the 814.71s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=26748)\u001b[0m No improvement since epoch 5: early stopping\u001b[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.46%)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8421\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t1.96s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 510.91s of the 809.61s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.31%)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8431\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t29.66s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 478.06s of the 776.77s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.27%)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8421\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t2.16s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.07s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 472.93s of the 771.63s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.29%)\n",
      "\u001b[36m(_ray_fit pid=27800)\u001b[0m No improvement since epoch 7: early stopping\n",
      "\u001b[36m(_ray_fit pid=17932)\u001b[0m No improvement since epoch 12: early stopping\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8383\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t53.23s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.5s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 416.79s of the 715.50s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.64%)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8433\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t51.8s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.07s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 361.96s of the 660.66s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=18576)\u001b[0m No improvement since epoch 15: early stopping\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.20%)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8417\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t1.06s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.03s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 358.02s of the 656.72s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.26%)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8424\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t10.31s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.25s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 344.45s of the 643.15s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8414\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t1.31s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.49s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 342.47s of the 641.18s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.20%)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8423\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t17.38s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 322.23s of the 620.93s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.29%)\n",
      "\u001b[36m(_ray_fit pid=31624)\u001b[0m No improvement since epoch 2: early stopping\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8382\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t9.06s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.15s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 310.21s of the 608.92s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.82%)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8417\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t20.05s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.05s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 286.65s of the 585.36s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=872)\u001b[0m No improvement since epoch 8: early stopping\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8391\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t3.21s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.57s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 282.63s of the 581.33s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.40%)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8434\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t3.36s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.15s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 276.31s of the 575.02s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.29%)\n",
      "\u001b[36m(_ray_fit pid=22564)\u001b[0m No improvement since epoch 3: early stopping\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8409\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t40.09s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.46s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 233.23s of the 531.94s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.37%)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8426\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t3.3s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.1s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 226.69s of the 525.40s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.37%)\n",
      "\u001b[36m(_ray_fit pid=17428)\u001b[0m No improvement since epoch 7: early stopping\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8424\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t2.33s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.07s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: CatBoost_r50_BAG_L1 ... Training model for up to 221.21s of the 519.92s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.21%)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.8438\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t19.69s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \t0.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m Fitting model: NeuralNetFastAI_r11_BAG_L1 ... Training model for up to 198.28s of the 496.99s of remaining time.\n",
      "\u001b[36m(_dystack pid=32520)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.29%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m timestamp \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mTabularPredictor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfraud\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m----> 4\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mholdout_frac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexcluded_model_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNN_TORCH\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKNN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCATBOOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m predictor \u001b[38;5;241m=\u001b[39m TabularPredictor(\n\u001b[0;32m     16\u001b[0m     label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfraud\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     30\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\core\\utils\\decorators.py:31\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     30\u001b[0m     gargs, gkwargs \u001b[38;5;241m=\u001b[39m g(\u001b[38;5;241m*\u001b[39mother_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1280\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[1;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, fit_full_last_level_weighted_ensemble, full_weighted_ensemble_additionally, dynamic_stacking, calibrate_decision_threshold, num_cpus, num_gpus, fit_strategy, memory_limit, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dynamic_stacking:\n\u001b[0;32m   1275\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog(\n\u001b[0;32m   1276\u001b[0m         \u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m   1277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDyStack is enabled (dynamic_stacking=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdynamic_stacking\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1279\u001b[0m     )\n\u001b[1;32m-> 1280\u001b[0m     num_stack_levels, time_limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamic_stacking\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mds_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1281\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   1282\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting main fit with num_stack_levels=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_stack_levels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1283\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mFor future fit calls on this dataset, you can skip DyStack to save time: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1284\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`predictor.fit(..., dynamic_stacking=False, num_stack_levels=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_stack_levels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1285\u001b[0m     )\n\u001b[0;32m   1287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (time_limit \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1380\u001b[0m, in \u001b[0;36mTabularPredictor._dynamic_stacking\u001b[1;34m(self, ag_fit_kwargs, ag_post_fit_kwargs, validation_procedure, detection_time_frac, holdout_frac, n_folds, n_repeats, memory_safe_fits, clean_up_fits, enable_ray_logging, enable_callbacks, holdout_data)\u001b[0m\n\u001b[0;32m   1377\u001b[0m         _, holdout_data, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_fit_data(train_data\u001b[38;5;241m=\u001b[39mX, tuning_data\u001b[38;5;241m=\u001b[39mholdout_data)\n\u001b[0;32m   1378\u001b[0m         ds_fit_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mds_fit_context\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ds_fit_context, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_fit_custom_ho\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1380\u001b[0m     stacked_overfitting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sub_fit_memory_save_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mds_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_ag_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_ag_post_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mholdout_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mholdout_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1390\u001b[0m     \u001b[38;5;66;03m# Holdout is false, use (repeated) cross-validation\u001b[39;00m\n\u001b[0;32m   1391\u001b[0m     is_stratified \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;129;01min\u001b[39;00m [REGRESSION, QUANTILE, SOFTCLASS]\n",
      "File \u001b[1;32mc:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1563\u001b[0m, in \u001b[0;36mTabularPredictor._sub_fit_memory_save_wrapper\u001b[1;34m(self, train_data, time_limit, time_start, ds_fit_kwargs, ag_fit_kwargs, ag_post_fit_kwargs, holdout_data)\u001b[0m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;66;03m# FIXME: For some reason ray does not treat `num_cpus` and `num_gpus` the same.\u001b[39;00m\n\u001b[0;32m   1550\u001b[0m \u001b[38;5;66;03m#  For `num_gpus`, the process will reserve the capacity and is unable to share it to child ray processes, causing a deadlock.\u001b[39;00m\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;66;03m#  For `num_cpus`, the value is completely ignored by children, and they can even use more num_cpus than the parent.\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;66;03m#  Because of this, num_gpus is set to 0 here to avoid a deadlock, but num_cpus does not need to be changed.\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m \u001b[38;5;66;03m#  For more info, refer to Ray documentation: https://docs.ray.io/en/latest/ray-core/tasks/nested-tasks.html#yielding-resources-while-blocked\u001b[39;00m\n\u001b[0;32m   1554\u001b[0m ref \u001b[38;5;241m=\u001b[39m sub_fit_caller\u001b[38;5;241m.\u001b[39moptions(num_cpus\u001b[38;5;241m=\u001b[39mnum_cpus, num_gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mremote(\n\u001b[0;32m   1555\u001b[0m     predictor\u001b[38;5;241m=\u001b[39mpredictor_ref,\n\u001b[0;32m   1556\u001b[0m     train_data\u001b[38;5;241m=\u001b[39mtrain_data_ref,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     holdout_data\u001b[38;5;241m=\u001b[39mholdout_data_ref,\n\u001b[0;32m   1562\u001b[0m )\n\u001b[1;32m-> 1563\u001b[0m finished, unfinished \u001b[38;5;241m=\u001b[39m \u001b[43m_ds_ray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mref\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m stacked_overfitting, ho_leaderboard, exception \u001b[38;5;241m=\u001b[39m _ds_ray\u001b[38;5;241m.\u001b[39mget(finished[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1566\u001b[0m \u001b[38;5;66;03m# TODO: This is present to ensure worker logs are properly logged and don't get skipped / printed out of order.\u001b[39;00m\n\u001b[0;32m   1567\u001b[0m \u001b[38;5;66;03m#  Ideally find a faster way to do this that doesn't introduce a 100 ms overhead.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\ray\\_private\\auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     20\u001b[0m     auto_init_ray()\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\ray\\_private\\client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\ray\\_private\\worker.py:2984\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(ray_waitables, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[0;32m   2982\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[0;32m   2983\u001b[0m timeout_milliseconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m-> 2984\u001b[0m ready_ids, remaining_ids \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mray_waitables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_milliseconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2990\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2991\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "File \u001b[1;32mpython\\ray\\_raylet.pyx:3816\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpython\\ray\\includes/common.pxi:79\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=32660)\u001b[0m No improvement since epoch 2: early stopping\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%m%d_%H%M\")\n",
    "# predictor = TabularPredictor(\n",
    "#     label=\"fraud\"\n",
    "# ).fit(\n",
    "#     train_data=train_df,\n",
    "#     holdout_frac=0.2,\n",
    "#     presets=\"best\",\n",
    "#     verbosity=2,\n",
    "#     excluded_model_types=[\"NN_TORCH\", \"KNN\", \"CATBOOST\"]\n",
    "# )\n",
    "\n",
    "\n",
    "predictor = TabularPredictor(\n",
    "    label=\"fraud\",\n",
    "    eval_metric=\"f1\",\n",
    "    problem_type=\"binary\",\n",
    "    path=f\"../AutogluonModels/Model_{timestamp}\"\n",
    ").fit(\n",
    "    train_data=train_df,\n",
    "    presets=\"best\",  # Or use \"high_quality_fast_inference_only_refit\" if you want lighter models\n",
    "    holdout_frac=0.2,\n",
    "    hyperparameters={\n",
    "        'GBM': {  # GBM = all gradient boosted models (LightGBM, CatBoost, XGBoost)\n",
    "            'ag_args_fit': {'num_gpus': 1}\n",
    "        }\n",
    "    },\n",
    "    included_model_types=[\"GBM\"],  # Only include GBMs\n",
    "    verbosity=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0d3effa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WeightedEnsemble_L3</td>\n",
       "      <td>0.372626</td>\n",
       "      <td>f1</td>\n",
       "      <td>12.431560</td>\n",
       "      <td>497.427063</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>2.927385</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NeuralNetFastAI_r143_BAG_L2</td>\n",
       "      <td>0.366606</td>\n",
       "      <td>f1</td>\n",
       "      <td>10.817639</td>\n",
       "      <td>375.918469</td>\n",
       "      <td>0.200080</td>\n",
       "      <td>10.570300</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NeuralNetFastAI_r102_BAG_L2</td>\n",
       "      <td>0.363663</td>\n",
       "      <td>f1</td>\n",
       "      <td>10.852048</td>\n",
       "      <td>374.476654</td>\n",
       "      <td>0.234489</td>\n",
       "      <td>9.128484</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NeuralNetFastAI_r156_BAG_L2</td>\n",
       "      <td>0.362118</td>\n",
       "      <td>f1</td>\n",
       "      <td>10.837050</td>\n",
       "      <td>376.610555</td>\n",
       "      <td>0.219491</td>\n",
       "      <td>11.262386</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NeuralNetFastAI_BAG_L2</td>\n",
       "      <td>0.361727</td>\n",
       "      <td>f1</td>\n",
       "      <td>10.985593</td>\n",
       "      <td>382.348483</td>\n",
       "      <td>0.368034</td>\n",
       "      <td>17.000314</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>ExtraTreesEntr_BAG_L1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.587954</td>\n",
       "      <td>0.699074</td>\n",
       "      <td>0.587954</td>\n",
       "      <td>0.699074</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>ExtraTrees_r126_BAG_L1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.758054</td>\n",
       "      <td>1.026744</td>\n",
       "      <td>0.758054</td>\n",
       "      <td>1.026744</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>XGBoost_r31_BAG_L1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.803297</td>\n",
       "      <td>2.912572</td>\n",
       "      <td>0.803297</td>\n",
       "      <td>2.912572</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>LightGBM_r96_BAG_L2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>f1</td>\n",
       "      <td>10.662560</td>\n",
       "      <td>367.158300</td>\n",
       "      <td>0.045001</td>\n",
       "      <td>1.810131</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>LightGBM_r196_BAG_L2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>f1</td>\n",
       "      <td>10.664571</td>\n",
       "      <td>367.936686</td>\n",
       "      <td>0.047012</td>\n",
       "      <td>2.588516</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model  score_val eval_metric  pred_time_val  \\\n",
       "0            WeightedEnsemble_L3   0.372626          f1      12.431560   \n",
       "1    NeuralNetFastAI_r143_BAG_L2   0.366606          f1      10.817639   \n",
       "2    NeuralNetFastAI_r102_BAG_L2   0.363663          f1      10.852048   \n",
       "3    NeuralNetFastAI_r156_BAG_L2   0.362118          f1      10.837050   \n",
       "4         NeuralNetFastAI_BAG_L2   0.361727          f1      10.985593   \n",
       "..                           ...        ...         ...            ...   \n",
       "123        ExtraTreesEntr_BAG_L1   0.000000          f1       0.587954   \n",
       "124       ExtraTrees_r126_BAG_L1   0.000000          f1       0.758054   \n",
       "125           XGBoost_r31_BAG_L1   0.000000          f1       0.803297   \n",
       "126          LightGBM_r96_BAG_L2   0.000000          f1      10.662560   \n",
       "127         LightGBM_r196_BAG_L2   0.000000          f1      10.664571   \n",
       "\n",
       "       fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  \\\n",
       "0    497.427063                0.004000           2.927385            3   \n",
       "1    375.918469                0.200080          10.570300            2   \n",
       "2    374.476654                0.234489           9.128484            2   \n",
       "3    376.610555                0.219491          11.262386            2   \n",
       "4    382.348483                0.368034          17.000314            2   \n",
       "..          ...                     ...                ...          ...   \n",
       "123    0.699074                0.587954           0.699074            1   \n",
       "124    1.026744                0.758054           1.026744            1   \n",
       "125    2.912572                0.803297           2.912572            1   \n",
       "126  367.158300                0.045001           1.810131            2   \n",
       "127  367.936686                0.047012           2.588516            2   \n",
       "\n",
       "     can_infer  fit_order  \n",
       "0         True        128  \n",
       "1         True        121  \n",
       "2         True        107  \n",
       "3         True        123  \n",
       "4         True         96  \n",
       "..         ...        ...  \n",
       "123       True          7  \n",
       "124       True         80  \n",
       "125       True         62  \n",
       "126       True        103  \n",
       "127       True        124  \n",
       "\n",
       "[128 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = predictor.leaderboard()\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03785190",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # MonthDay_HourMinute format\n",
    "timestamp = datetime.now().strftime(\"%m%d_%H%M\")\n",
    "\n",
    "# 4. Predict on the test set\n",
    "test_df = pd.read_csv('../Data/processed/0427_01/test_2025.csv')\n",
    "predictions = predictor.predict(test_df)\n",
    "\n",
    "# 5. Save predictions to CSV\n",
    "submission = pd.DataFrame({\n",
    "    \"claim_number\": test_id,  # Important: use the original claim_number\n",
    "    \"fraud\": predictions                      # Your predicted fraud labels (0 or 1)\n",
    "})\n",
    "submission.to_csv(f\"../Submit/submissions/submission_{timestamp}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "198ca370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 47 features using 1000 rows with 2 shuffle sets...\n",
      "\t462.5s\t= Expected runtime (231.25s per shuffle set)\n",
      "\t99.2s\t= Actual runtime (Completed 2 of 2 shuffle sets)\n"
     ]
    }
   ],
   "source": [
    "importances = predictor.feature_importance(data=train_df, subsample_size=1000, num_shuffle_sets=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
