{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "499c1ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 1. Import library\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2784c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", message=\".*load_learner.*insecure pickle.*\")\n",
    "\n",
    "# 2. Load the data\n",
    "train_df = pd.read_csv('../Data/processed/0427_01/train_2025.csv') \n",
    "test_df = pd.read_csv('../Data/processed/0427_01/test_2025.csv') \n",
    "\n",
    "train_df.drop(columns=\"claim_number\", inplace=True)\n",
    "test_id = test_df['claim_number']\n",
    "test_df.drop(columns=[\"claim_number\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae6d05d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['median_home_value', \n",
    "                'occupied_housing_units', \n",
    "                'housing_units', \n",
    "                'population_density', \n",
    "                'population', \n",
    "                'vacancy_rate', \n",
    "                'pop_per_occupied_housing_units', \n",
    "                'home_value_v_median_household_income', \n",
    "                'log_occupied_housing_per_sqmi']\n",
    "\n",
    "train_df.drop(columns = drop_columns, inplace=True)\n",
    "test_df.drop(columns = drop_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "401332eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preset alias specified: 'best' maps to 'best_quality'.\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.11.12\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          16\n",
      "Memory Avail:       15.63 GB / 31.93 GB (49.0%)\n",
      "Disk Space Avail:   284.22 GB / 935.97 GB (30.4%)\n",
      "===================================================\n",
      "Presets specified: ['best']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n",
      "2025-04-28 15:34:23,736\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-04-28 15:34:26,302\tINFO worker.py:1810 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"c:\\Users\\yangs\\Documents\\Python Projects\\NESS-2025\\AutogluonModels\\Model_0428_1534\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m Beginning AutoGluon training ... Time limit = 896s\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m AutoGluon will save models to \"c:\\Users\\yangs\\Documents\\Python Projects\\NESS-2025\\AutogluonModels\\Model_0428_1534\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m Train Data Rows:    16000\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m Train Data Columns: 47\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m Label Column:       fraud\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m Problem Type:       binary\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \tAvailable Memory:                    15395.91 MB\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \tTrain Data (Original)  Memory Usage: 8.24 MB (0.1% of available memory)\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t\t\tNote: Converting 16 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t\t('bool', [])   :  1 | ['relative_income_ind']\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t\t('float', [])  :  7 | ['marital_status', 'witness_present_ind', 'claim_est_payout', 'vehicle_price', 'vehicle_weight', ...]\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t\t('int', [])    : 36 | ['age_of_driver', 'gender', 'safty_rating', 'annual_income', 'high_education_ind', ...]\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t\t('object', []) :  3 | ['zipcode_type', 'state', 'county']\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t\t('category', [])  :  3 | ['zipcode_type', 'state', 'county']\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t\t('float', [])     :  7 | ['marital_status', 'witness_present_ind', 'claim_est_payout', 'vehicle_price', 'vehicle_weight', ...]\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t\t('int', [])       : 21 | ['age_of_driver', 'safty_rating', 'annual_income', 'past_num_of_claims', 'liab_prct', ...]\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t\t('int', ['bool']) : 16 | ['gender', 'high_education_ind', 'address_change_ind', 'living_status', 'policy_report_filed_ind', ...]\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t0.2s = Fit runtime\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t47 features in original data used to generate 47 features in processed data.\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \tTrain Data (Processed) Memory Usage: 3.71 MB (0.0% of available memory)\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m Data preprocessing and feature engineering runtime = 0.18s ...\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'f1'\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t'GBM': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t'CAT': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t'XGB': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m Included models: ['GBM', 'CAT', 'XGB'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m Fitting 3 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 596.77s of the 895.37s of remaining time.\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \tWarning: Exception caused LightGBM_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t\tSpecified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m   File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2106, in _train_and_save\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m   File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1993, in _train_single\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m   File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 914, in fit\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m     kwargs = self._preprocess_fit_args(**kwargs)\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m   File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 566, in _preprocess_fit_args\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m     kwargs = self._preprocess_fit_resources(**kwargs)\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m   File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 816, in _preprocess_fit_resources\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m     return self._calculate_total_resources(silent=silent, total_resources=total_resources, parallel_hpo=parallel_hpo, **kwargs)\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m   File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 697, in _calculate_total_resources\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m     user_specified_lower_level_num_gpus = self._process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble(\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m   File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 642, in _process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m     assert user_specified_model_level_resource <= system_resource, f\"Specified {resource_type} per model base is more than the total: {system_resource}\"\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m AssertionError: Specified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 594.60s of the 893.21s of remaining time.\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \tWarning: Exception caused CatBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t\tSpecified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m   File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2106, in _train_and_save\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m   File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1993, in _train_single\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m   File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 914, in fit\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m     kwargs = self._preprocess_fit_args(**kwargs)\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m   File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 566, in _preprocess_fit_args\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m     kwargs = self._preprocess_fit_resources(**kwargs)\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m   File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 816, in _preprocess_fit_resources\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m     return self._calculate_total_resources(silent=silent, total_resources=total_resources, parallel_hpo=parallel_hpo, **kwargs)\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m   File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 697, in _calculate_total_resources\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m     user_specified_lower_level_num_gpus = self._process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble(\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m   File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 642, in _process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m     assert user_specified_model_level_resource <= system_resource, f\"Specified {resource_type} per model base is more than the total: {system_resource}\"\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m AssertionError: Specified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 594.51s of the 893.11s of remaining time.\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m \t\tSpecified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m   File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2106, in _train_and_save\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m   File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1993, in _train_single\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m   File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 914, in fit\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m     kwargs = self._preprocess_fit_args(**kwargs)\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m   File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 566, in _preprocess_fit_args\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m     kwargs = self._preprocess_fit_resources(**kwargs)\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m   File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 816, in _preprocess_fit_resources\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m     return self._calculate_total_resources(silent=silent, total_resources=total_resources, parallel_hpo=parallel_hpo, **kwargs)\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m   File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 697, in _calculate_total_resources\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m     user_specified_lower_level_num_gpus = self._process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble(\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m   File \"c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 642, in _process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m     assert user_specified_model_level_resource <= system_resource, f\"Specified {resource_type} per model base is more than the total: {system_resource}\"\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m AssertionError: Specified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m No base models to train on, skipping auxiliary stack level 2...\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m No base models to train on, skipping stack level 2...\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m No base models to train on, skipping auxiliary stack level 3...\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m Warning: AutoGluon did not successfully train any models\n",
      "\u001b[36m(_dystack pid=32724)\u001b[0m AutoGluon training complete, total runtime = 2.55s ... Best model: None\n",
      "Warning: Exception encountered during DyStack sub-fit:\n",
      "\tNo models were trained successfully during fit(). Inspect the log output or increase verbosity to determine why no models were fit. Alternatively, set `raise_on_no_models_fitted` to False during the fit call.\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t9s\t = DyStack   runtime |\t3591s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 3591s\n",
      "AutoGluon will save models to \"c:\\Users\\yangs\\Documents\\Python Projects\\NESS-2025\\AutogluonModels\\Model_0428_1534\"\n",
      "Train Data Rows:    18000\n",
      "Train Data Columns: 47\n",
      "Label Column:       fraud\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    15151.57 MB\n",
      "\tTrain Data (Original)  Memory Usage: 9.27 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 16 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['relative_income_ind']\n",
      "\t\t('float', [])  :  7 | ['marital_status', 'witness_present_ind', 'claim_est_payout', 'vehicle_price', 'vehicle_weight', ...]\n",
      "\t\t('int', [])    : 36 | ['age_of_driver', 'gender', 'safty_rating', 'annual_income', 'high_education_ind', ...]\n",
      "\t\t('object', []) :  3 | ['zipcode_type', 'state', 'county']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :  3 | ['zipcode_type', 'state', 'county']\n",
      "\t\t('float', [])     :  7 | ['marital_status', 'witness_present_ind', 'claim_est_payout', 'vehicle_price', 'vehicle_weight', ...]\n",
      "\t\t('int', [])       : 21 | ['age_of_driver', 'safty_rating', 'annual_income', 'past_num_of_claims', 'liab_prct', ...]\n",
      "\t\t('int', ['bool']) : 16 | ['gender', 'high_education_ind', 'address_change_ind', 'living_status', 'policy_report_filed_ind', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t47 features in original data used to generate 47 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 4.17 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.18s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'CAT': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "\t'XGB': [{'ag_args_fit': {'num_gpus': 1}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Included models: ['GBM', 'CAT', 'XGB'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "Fitting 3 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2393.25s of the 3590.77s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.23%)\n",
      "\t0.0951\t = Validation score   (f1)\n",
      "\t69.8s\t = Training   runtime\n",
      "\t0.53s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 2318.46s of the 3515.97s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.29%)\n",
      "\t0.0241\t = Validation score   (f1)\n",
      "\t142.47s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 2173.95s of the 3371.47s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.35%)\n",
      "\t0.1166\t = Validation score   (f1)\n",
      "\t50.76s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 3318.59s of remaining time.\n",
      "\tEnsemble Weights: {'XGBoost_BAG_L1': 1.0}\n",
      "\t0.1166\t = Validation score   (f1)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Included models: ['GBM', 'CAT', 'XGB'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "Fitting 3 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 3318.22s of the 3318.22s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.24%)\n",
      "\t0.0901\t = Validation score   (f1)\n",
      "\t63.08s\t = Training   runtime\n",
      "\t0.41s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 3252.98s of the 3252.98s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.30%)\n",
      "\t0.0083\t = Validation score   (f1)\n",
      "\t82.63s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 3168.26s of the 3168.26s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.37%)\n",
      "\t0.1328\t = Validation score   (f1)\n",
      "\t76.49s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 3089.66s of remaining time.\n",
      "\tEnsemble Weights: {'XGBoost_BAG_L2': 1.0}\n",
      "\t0.1328\t = Validation score   (f1)\n",
      "\t0.72s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 502.05s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 2050.7 rows/s (2250 batch size)\n",
      "Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
      "Calibrating decision threshold to optimize metric f1 | Checking 51 thresholds...\n",
      "Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
      "\tBase Threshold: 0.500\t| val: 0.1328\n",
      "\tBest Threshold: 0.105\t| val: 0.3495\n",
      "Updating predictor.decision_threshold from 0.5 -> 0.10499999999999997\n",
      "\tThis will impact how prediction probabilities are converted to predictions in binary classification.\n",
      "\tPrediction probabilities of the positive class >0.10499999999999997 will be predicted as the positive class (1). This can significantly impact metric scores.\n",
      "\tYou can update this value via `predictor.set_decision_threshold`.\n",
      "\tYou can calculate an optimal decision threshold on the validation data via `predictor.calibrate_decision_threshold()`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\yangs\\Documents\\Python Projects\\NESS-2025\\AutogluonModels\\Model_0428_1534\")\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%m%d_%H%M\")\n",
    "\n",
    "\n",
    "predictor = TabularPredictor(\n",
    "    label=\"fraud\",\n",
    "    eval_metric=\"f1\",\n",
    "    problem_type=\"binary\",\n",
    "    path=f\"../AutogluonModels/Model_{timestamp}\"\n",
    ").fit(\n",
    "    train_data=train_df,\n",
    "    presets=\"best\",  # Or use \"high_quality_fast_inference_only_refit\" if you want lighter models\n",
    "    holdout_frac=0.2,\n",
    "    hyperparameters={\n",
    "        'GBM': { 'ag_args_fit': {'num_gpus': 1} },            # LightGBM\n",
    "        'CAT': { 'ag_args_fit': {'num_gpus': 1} },            # CatBoost\n",
    "        'XGB': { 'ag_args_fit': {'num_gpus': 1} },            # XGBoost\n",
    "    },\n",
    "    included_model_types=[\"GBM\", \"CAT\", \"XGB\"],  # Only include GBMs\n",
    "    verbosity=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0d3effa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost_BAG_L2</td>\n",
       "      <td>0.132773</td>\n",
       "      <td>f1</td>\n",
       "      <td>1.096547</td>\n",
       "      <td>339.533042</td>\n",
       "      <td>0.333666</td>\n",
       "      <td>76.490993</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L3</td>\n",
       "      <td>0.132773</td>\n",
       "      <td>f1</td>\n",
       "      <td>1.101550</td>\n",
       "      <td>340.257819</td>\n",
       "      <td>0.005003</td>\n",
       "      <td>0.724777</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost_BAG_L1</td>\n",
       "      <td>0.116592</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.192348</td>\n",
       "      <td>50.764673</td>\n",
       "      <td>0.192348</td>\n",
       "      <td>50.764673</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.116592</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.196348</td>\n",
       "      <td>51.108705</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.344032</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>0.095149</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.530352</td>\n",
       "      <td>69.804777</td>\n",
       "      <td>0.530352</td>\n",
       "      <td>69.804777</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LightGBM_BAG_L2</td>\n",
       "      <td>0.090050</td>\n",
       "      <td>f1</td>\n",
       "      <td>1.169547</td>\n",
       "      <td>326.124204</td>\n",
       "      <td>0.406666</td>\n",
       "      <td>63.082155</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CatBoost_BAG_L1</td>\n",
       "      <td>0.024088</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.040181</td>\n",
       "      <td>142.472599</td>\n",
       "      <td>0.040181</td>\n",
       "      <td>142.472599</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CatBoost_BAG_L2</td>\n",
       "      <td>0.008336</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.801175</td>\n",
       "      <td>345.671312</td>\n",
       "      <td>0.038294</td>\n",
       "      <td>82.629263</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  score_val eval_metric  pred_time_val    fit_time  \\\n",
       "0       XGBoost_BAG_L2   0.132773          f1       1.096547  339.533042   \n",
       "1  WeightedEnsemble_L3   0.132773          f1       1.101550  340.257819   \n",
       "2       XGBoost_BAG_L1   0.116592          f1       0.192348   50.764673   \n",
       "3  WeightedEnsemble_L2   0.116592          f1       0.196348   51.108705   \n",
       "4      LightGBM_BAG_L1   0.095149          f1       0.530352   69.804777   \n",
       "5      LightGBM_BAG_L2   0.090050          f1       1.169547  326.124204   \n",
       "6      CatBoost_BAG_L1   0.024088          f1       0.040181  142.472599   \n",
       "7      CatBoost_BAG_L2   0.008336          f1       0.801175  345.671312   \n",
       "\n",
       "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                0.333666          76.490993            2       True   \n",
       "1                0.005003           0.724777            3       True   \n",
       "2                0.192348          50.764673            1       True   \n",
       "3                0.004000           0.344032            2       True   \n",
       "4                0.530352          69.804777            1       True   \n",
       "5                0.406666          63.082155            2       True   \n",
       "6                0.040181         142.472599            1       True   \n",
       "7                0.038294          82.629263            2       True   \n",
       "\n",
       "   fit_order  \n",
       "0          7  \n",
       "1          8  \n",
       "2          3  \n",
       "3          4  \n",
       "4          1  \n",
       "5          5  \n",
       "6          2  \n",
       "7          6  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = predictor.leaderboard()\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03785190",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # MonthDay_HourMinute format\n",
    "timestamp = datetime.now().strftime(\"%m%d_%H%M\")\n",
    "\n",
    "# 4. Predict on the test set\n",
    "test_df = pd.read_csv('../Data/processed/0427_01/test_2025.csv')\n",
    "predictions = predictor.predict(test_df)\n",
    "\n",
    "# 5. Save predictions to CSV\n",
    "submission = pd.DataFrame({\n",
    "    \"claim_number\": test_id,  # Important: use the original claim_number\n",
    "    \"fraud\": predictions                      # Your predicted fraud labels (0 or 1)\n",
    "})\n",
    "submission.to_csv(f\"../Submit/submissions/submission_{timestamp}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "198ca370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 47 features using 2000 rows with 3 shuffle sets...\n",
      "\t116.24s\t= Expected runtime (38.75s per shuffle set)\n",
      "\t40.75s\t= Actual runtime (Completed 3 of 3 shuffle sets)\n"
     ]
    }
   ],
   "source": [
    "importances = predictor.feature_importance(data=train_df, subsample_size=2000, num_shuffle_sets=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
