{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d89ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yangs\\.conda\\envs\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Add the grandparent directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../')))\n",
    "from Utils import FE_helper as FE\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f68f11e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the data\n",
    "train_df = pd.read_csv('../Data/original/train_2025.csv') \n",
    "test_df = pd.read_csv('../Data/original/test_2025.csv')\n",
    "\n",
    "train_df = FE.add_features(train_df)\n",
    "test_df = FE.add_features(test_df)\n",
    "\n",
    "test_id = test_df['claim_number']\n",
    "train_id = train_df['claim_number']\n",
    "target = train_df['fraud']\n",
    "\n",
    "ignore_var = ['claim_date.dayofweek', 'claim_date.month', 'claim_date.day', 'claim_date.weekofyear']\n",
    "train_df = FE.drop_ignored_columns(train_df, ignore_var)\n",
    "test_df = FE.drop_ignored_columns(test_df, ignore_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f0044e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, full_train_df, kfoldcv = 20, drop = []):\n",
    "    ignore_var = ['claim_number', 'fraud']\n",
    "    y = full_train_df['fraud']\n",
    "\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'None',\n",
    "        'is_unbalance': True,\n",
    "        'boosting': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 50, 150),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.3, 0.9),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.3, 0.9),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=kfoldcv, shuffle=True)\n",
    "    best_thresholds = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(full_train_df, y):\n",
    "        raw_train = full_train_df.iloc[train_idx].copy()\n",
    "        raw_val = full_train_df.iloc[val_idx].copy()\n",
    "\n",
    "        # Preprocess inside the fold (train fitted only on fold-train)\n",
    "        X_train_df, X_val_df = FE.preprocess_train_test(raw_train, raw_val, ignore_var=ignore_var)\n",
    "        X_train_df.drop(columns=drop, inplace=True, errors='ignore')\n",
    "        X_val_df.drop(columns=drop, inplace=True, errors='ignore')\n",
    "        \n",
    "        y_train = raw_train['fraud']\n",
    "        y_val = raw_val['fraud']\n",
    "\n",
    "        lgb_train = lgb.Dataset(X_train_df, y_train)\n",
    "        lgb_val = lgb.Dataset(X_val_df, y_val, reference=lgb_train)\n",
    "\n",
    "        model = lgb.train(params, lgb_train, num_boost_round=500, valid_sets=[lgb_val])\n",
    "        importance = dict(zip(model.feature_name(), model.feature_importance()))\n",
    "        trial.set_user_attr('feature_importance', importance)\n",
    "\n",
    "        # Predict + threshold tune\n",
    "        probs = model.predict(X_val_df)\n",
    "        thresholds = np.linspace(0.1, 0.9, 50)\n",
    "        f1s = [f1_score(y_val, probs > t) for t in thresholds]\n",
    "\n",
    "        best_f1 = max(f1s)\n",
    "        best_threshold = thresholds[np.argmax(f1s)]\n",
    "\n",
    "        f1_scores.append(best_f1)\n",
    "        best_thresholds.append(best_threshold)\n",
    "        mean_threshold = np.mean(best_thresholds)\n",
    "        \n",
    "        trial.set_user_attr('mean_threshold', mean_threshold)\n",
    "        \n",
    "    return np.mean(f1_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74af606c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-03 10:38:48,231] A new study created in memory with name: no-name-de121531-4bfe-4ee0-8ed0-dae6e06627c2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Round 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-03 10:39:24,481] Trial 0 finished with value: 0.3537892750350373 and parameters: {'num_leaves': 125, 'feature_fraction': 0.8166670940553167, 'bagging_fraction': 0.6612684859644169, 'bagging_freq': 7, 'learning_rate': 0.0327414128098123}. Best is trial 0 with value: 0.3537892750350373.\n",
      "[I 2025-05-03 10:39:54,395] Trial 1 finished with value: 0.33917051382747815 and parameters: {'num_leaves': 104, 'feature_fraction': 0.4708780309086963, 'bagging_fraction': 0.3591659642061297, 'bagging_freq': 5, 'learning_rate': 0.05019389651661707}. Best is trial 0 with value: 0.3537892750350373.\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "low_importance_features = []\n",
    "best_f1_overall = 0\n",
    "round_logs = []\n",
    "\n",
    "# Setup log file name\n",
    "timestamp = time.strftime(\"%m%d_%H%M\")\n",
    "log_file = f\"logs/feature_pruning_log_{timestamp}.csv\"\n",
    "\n",
    "# Loop parameters\n",
    "max_rounds = 25\n",
    "min_features = 40\n",
    "n_trials_per_round = 20\n",
    "cutoff_quantile = 0.05\n",
    "\n",
    "for round_idx in range(max_rounds):\n",
    "    print(f\"\\n=== Round {round_idx + 1} ===\")\n",
    "    \n",
    "    # Run Optuna\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, train_df, drop=low_importance_features),\n",
    "                   n_trials=n_trials_per_round)\n",
    "    \n",
    "    best_f1 = study.best_value\n",
    "    print(f\"Best F1 this round: {best_f1:.4f}\")\n",
    "    \n",
    "    # Track best overall F1\n",
    "    if best_f1 > best_f1_overall:\n",
    "        best_f1_overall = best_f1\n",
    "    \n",
    "    # Collect feature importance\n",
    "    feature_scores = {}\n",
    "    for t in study.trials:\n",
    "        imp = t.user_attrs.get('feature_importance')\n",
    "        if imp is not None:\n",
    "            for k, v in imp.items():\n",
    "                feature_scores.setdefault(k, []).append(v)\n",
    "    agg_importance = {\n",
    "        k: np.mean(v)\n",
    "        for k, v in feature_scores.items()\n",
    "    }\n",
    "    importance_df = pd.DataFrame(list(agg_importance.items()), columns=['feature', 'mean_importance'])\n",
    "    \n",
    "    # Identify low-importance features\n",
    "    threshold = importance_df['mean_importance'].quantile(cutoff_quantile)\n",
    "    dropped_features = importance_df[\n",
    "        importance_df['mean_importance'] <= threshold\n",
    "    ]['feature'].tolist()\n",
    "    \n",
    "    print(f\"Dropping {len(dropped_features)} features (bottom {cutoff_quantile * 100:.1f}%)\")\n",
    "    print(\"Features to drop:\", dropped_features)\n",
    "    \n",
    "    # Update drop list\n",
    "    low_importance_features.extend(dropped_features)\n",
    "    \n",
    "    # Log round\n",
    "    round_logs.append({\n",
    "        'round': round_idx + 1,\n",
    "        'f1_this_round': best_f1,\n",
    "        'best_f1_overall': best_f1_overall,\n",
    "        'num_features_remaining': len(importance_df) - len(low_importance_features),\n",
    "        'features_dropped_this_round': dropped_features\n",
    "    })\n",
    "    \n",
    "    # Save to CSV after each round\n",
    "    pd.DataFrame(round_logs).to_csv(log_file, index=False)\n",
    "    print(f\"Log saved to {log_file}\")\n",
    "    \n",
    "    # Check stopping condition\n",
    "    num_features_left = len(importance_df) - len(dropped_features)\n",
    "    if num_features_left <= min_features:\n",
    "        print(f\"Stopping: reached minimum feature count ({min_features})\")\n",
    "        break\n",
    "\n",
    "print(\"\\n=== Finished all rounds ===\")\n",
    "print(f\"Best overall F1: {best_f1_overall:.4f}\")\n",
    "print(f\"Final log saved to {log_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37998bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-03 10:11:26,584] A new study created in memory with name: no-name-3afe2023-b792-4039-9e0b-3735a17e11d3\n",
      "[I 2025-05-03 10:11:48,849] Trial 0 finished with value: 0.37119214646493703 and parameters: {'num_leaves': 60, 'feature_fraction': 0.5738989726989637, 'bagging_fraction': 0.563220854609991, 'bagging_freq': 4, 'learning_rate': 0.021260244717947515}. Best is trial 0 with value: 0.37119214646493703.\n",
      "[I 2025-05-03 10:12:08,378] Trial 1 finished with value: 0.37341871426562917 and parameters: {'num_leaves': 56, 'feature_fraction': 0.4428481867959043, 'bagging_fraction': 0.32838859534683584, 'bagging_freq': 4, 'learning_rate': 0.013870893199460107}. Best is trial 1 with value: 0.37341871426562917.\n",
      "[I 2025-05-03 10:12:45,191] Trial 2 finished with value: 0.3610673093366913 and parameters: {'num_leaves': 136, 'feature_fraction': 0.6634225068051238, 'bagging_fraction': 0.8953943286393737, 'bagging_freq': 6, 'learning_rate': 0.0441875432607759}. Best is trial 1 with value: 0.37341871426562917.\n",
      "[I 2025-05-03 10:13:19,744] Trial 3 finished with value: 0.3553270165456056 and parameters: {'num_leaves': 121, 'feature_fraction': 0.7705502405615003, 'bagging_fraction': 0.7604662537938363, 'bagging_freq': 3, 'learning_rate': 0.046484756296401634}. Best is trial 1 with value: 0.37341871426562917.\n",
      "[I 2025-05-03 10:13:58,577] Trial 4 finished with value: 0.34668509550706406 and parameters: {'num_leaves': 148, 'feature_fraction': 0.3190468472387284, 'bagging_fraction': 0.6830717172588023, 'bagging_freq': 3, 'learning_rate': 0.054620768770605094}. Best is trial 1 with value: 0.37341871426562917.\n",
      "[I 2025-05-03 10:14:30,431] Trial 5 finished with value: 0.3688461262844768 and parameters: {'num_leaves': 108, 'feature_fraction': 0.47721452242276385, 'bagging_fraction': 0.42704672947851297, 'bagging_freq': 7, 'learning_rate': 0.024577152097149186}. Best is trial 1 with value: 0.37341871426562917.\n",
      "[I 2025-05-03 10:14:58,256] Trial 6 finished with value: 0.36728196769363197 and parameters: {'num_leaves': 107, 'feature_fraction': 0.30795428440494504, 'bagging_fraction': 0.7773630742528137, 'bagging_freq': 6, 'learning_rate': 0.026358269404469836}. Best is trial 1 with value: 0.37341871426562917.\n",
      "[I 2025-05-03 10:15:19,145] Trial 7 finished with value: 0.3532590642508595 and parameters: {'num_leaves': 61, 'feature_fraction': 0.7289704429103443, 'bagging_fraction': 0.6634572019701316, 'bagging_freq': 1, 'learning_rate': 0.06372312926531278}. Best is trial 1 with value: 0.37341871426562917.\n",
      "[I 2025-05-03 10:15:56,672] Trial 8 finished with value: 0.35664689314325726 and parameters: {'num_leaves': 128, 'feature_fraction': 0.7501198042009689, 'bagging_fraction': 0.5679519466074765, 'bagging_freq': 3, 'learning_rate': 0.0434822141396161}. Best is trial 1 with value: 0.37341871426562917.\n",
      "[I 2025-05-03 10:16:22,560] Trial 9 finished with value: 0.36302432227956266 and parameters: {'num_leaves': 84, 'feature_fraction': 0.7117941747554457, 'bagging_fraction': 0.7091939635762645, 'bagging_freq': 1, 'learning_rate': 0.035638457037808095}. Best is trial 1 with value: 0.37341871426562917.\n",
      "[I 2025-05-03 10:16:46,772] Trial 10 finished with value: 0.3309876453498663 and parameters: {'num_leaves': 81, 'feature_fraction': 0.4594361366271614, 'bagging_fraction': 0.3237833498555402, 'bagging_freq': 10, 'learning_rate': 0.08854760374309045}. Best is trial 1 with value: 0.37341871426562917.\n",
      "[I 2025-05-03 10:17:06,322] Trial 11 finished with value: 0.37313139300756715 and parameters: {'num_leaves': 50, 'feature_fraction': 0.5388245395397009, 'bagging_fraction': 0.4951994345041527, 'bagging_freq': 4, 'learning_rate': 0.016601457394935744}. Best is trial 1 with value: 0.37341871426562917.\n",
      "[I 2025-05-03 10:17:26,713] Trial 12 finished with value: 0.37132969031196855 and parameters: {'num_leaves': 53, 'feature_fraction': 0.893578538727851, 'bagging_fraction': 0.4158553232814547, 'bagging_freq': 8, 'learning_rate': 0.01242158991100988}. Best is trial 1 with value: 0.37341871426562917.\n",
      "[I 2025-05-03 10:17:51,455] Trial 13 finished with value: 0.3763167692043489 and parameters: {'num_leaves': 75, 'feature_fraction': 0.4632205193129215, 'bagging_fraction': 0.45056718969366055, 'bagging_freq': 4, 'learning_rate': 0.005996539003885148}. Best is trial 13 with value: 0.3763167692043489.\n",
      "[I 2025-05-03 10:18:15,071] Trial 14 finished with value: 0.3756010424723624 and parameters: {'num_leaves': 76, 'feature_fraction': 0.4106624086737183, 'bagging_fraction': 0.3128044042059545, 'bagging_freq': 5, 'learning_rate': 0.005301654839749337}. Best is trial 13 with value: 0.3763167692043489.\n",
      "[I 2025-05-03 10:18:40,518] Trial 15 finished with value: 0.3781142816404603 and parameters: {'num_leaves': 82, 'feature_fraction': 0.3956303485523327, 'bagging_fraction': 0.41663462240856264, 'bagging_freq': 8, 'learning_rate': 0.0058344028862657715}. Best is trial 15 with value: 0.3781142816404603.\n",
      "[I 2025-05-03 10:19:08,035] Trial 16 finished with value: 0.34325141630360717 and parameters: {'num_leaves': 92, 'feature_fraction': 0.3867424521104916, 'bagging_fraction': 0.4569953623699168, 'bagging_freq': 9, 'learning_rate': 0.07200855961499814}. Best is trial 15 with value: 0.3781142816404603.\n",
      "[I 2025-05-03 10:19:34,828] Trial 17 finished with value: 0.37564294597471654 and parameters: {'num_leaves': 73, 'feature_fraction': 0.5106394218912735, 'bagging_fraction': 0.5006996125755347, 'bagging_freq': 7, 'learning_rate': 0.007047167688083377}. Best is trial 15 with value: 0.3781142816404603.\n",
      "[I 2025-05-03 10:20:02,345] Trial 18 finished with value: 0.3586275692591313 and parameters: {'num_leaves': 95, 'feature_fraction': 0.6147108545183384, 'bagging_fraction': 0.40595349253129903, 'bagging_freq': 10, 'learning_rate': 0.02967534765509356}. Best is trial 15 with value: 0.3781142816404603.\n",
      "[I 2025-05-03 10:20:24,029] Trial 19 finished with value: 0.3446303279946099 and parameters: {'num_leaves': 71, 'feature_fraction': 0.3910443579038599, 'bagging_fraction': 0.612317671177206, 'bagging_freq': 8, 'learning_rate': 0.08361299045939652}. Best is trial 15 with value: 0.3781142816404603.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 this round: 0.3781\n"
     ]
    }
   ],
   "source": [
    "feature_prune = pd.read_csv('logs/feature_pruning_log_0503_0218.csv')\n",
    "LIF = FE.get_cumulative_dropped_features(feature_prune, 4)\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(lambda trial: objective(trial, train_df, kfoldcv=20, drop=LIF),\n",
    "                n_trials=20)\n",
    "\n",
    "best_f1 = study.best_value\n",
    "print(f\"Best F1 this round: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c65f3a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold = study.best_trial.user_attrs['mean_threshold']\n",
    "best_params = study.best_params\n",
    "\n",
    "best_params.update({\n",
    "    'objective': 'binary',\n",
    "    'metric': 'None',\n",
    "    'is_unbalance': True,\n",
    "    'boosting': 'gbdt',\n",
    "    'verbose': -1\n",
    "})\n",
    "\n",
    "\n",
    "X_train_df, X_test_df = FE.preprocess_train_test(train_df, test_df, ignore_var=['claim_number', 'fraud'])\n",
    "\n",
    "X_train_df.drop(columns=LIF, inplace=True, errors='ignore')\n",
    "X_test_df.drop(columns=LIF, inplace=True, errors='ignore')\n",
    "\n",
    "lgb_train_full = lgb.Dataset(X_train_df, target)\n",
    "\n",
    "final_model = lgb.train(best_params, lgb_train_full, num_boost_round=500)\n",
    "\n",
    "probs_test = final_model.predict(X_test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c0637ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%m%d_%H%M')\n",
    "\n",
    "final_preds = (probs_test > best_threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'claim_number': test_id,\n",
    "    'fraud': final_preds\n",
    "})\n",
    "\n",
    "submission.to_csv(f'../Submit/submission_{timestamp}.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
